{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renmsd/portfoilo/blob/main/Gen%20Ai/LLM/RAG/Copy_of_RAG_Conquering_the_Last_Mile(GOOGLE).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYFJTMjrZE5E"
      },
      "source": [
        "# RAG: Conquering the Last Mile\n",
        "\n",
        "\n",
        "**Workshop Objective:** Navigating the final steps from a working prototype to a production service is often the hardest part of the journey. This workshop is your guide for conquering \"the last mile\" of RAG development. You will learn the essential techniques to **validate, secure, and monitor** your RAG application to make it reliable and production-ready.\n",
        "\n",
        "**Our Journey:**\n",
        "1.  **The Prototype:** We'll quickly build a working RAG system using a common knowledge dataset (Planets of the Solar System).\n",
        "2.  **The Last Mile - Step 1: VALIDATE:** We'll programmatically evaluate our prototype's performance using the Vertex AI Evaluation Service to get baseline quality metrics. *Is our prototype actually any good?*\n",
        "3.  **The Last Mile - Step 2: SECURE:** We'll add a critical production feature: access control. We will modify our RAG system to respect user permissions and only return data they are authorized to see. *Is our system safe to use?*\n",
        "4.  **The Last Mile - Step 3: MONITOR:** We'll implement logging to capture a record of every query, its result, and user feedback. This is the foundation for monitoring system health and performance over time. *Do we know how our system is performing in the wild?*\n",
        "5.  **Closing the Loop (TUNE & RE-VALIDATE):** Using our validation framework, we'll tune a key parameter (chunk size), rebuild our system, and re-evaluate to measure the impact of our change, demonstrating a data-driven approach to improvement.\n",
        "\n",
        "\n",
        "Workshop Slides  : https://docs.google.com/presentation/d/1w5aLc3aopumFoLeyX7J8hFH3Pb4gFPlyaJMh13hNGzA/edit?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0TfFxc0ZE5F"
      },
      "source": [
        "## Step 0: Setup and Authentication\n",
        "\n",
        "First, we'll authenticate our Colab environment to access our Google Cloud project and install the required Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing - 1\")\n",
        "!pip install -q google-cloud-aiplatform[evaluation]\n",
        "print(\"Installing - 2 \")\n",
        "!pip install -q --upgrade google-colab google-cloud-bigquery google-cloud-aiplatform langchain \"langchain-google-vertexai>1.0.0\"\n",
        "# pandas pyarrow\n",
        "print(\"Installing - 3 \")\n",
        "!pip install -q --upgrade pandas pyarrow\n",
        "print(\"Installing - 4 \")\n",
        "!pip install -q google-cloud-aiplatform[evaluation] ragas rouge_score\n",
        "# Restart runtime to load the newly installed package\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "i7739-unGLTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91886910-32eb-49c9-d731-c9439f9d9b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing - 1\n",
            "Installing - 2 \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.31 requires requests<3.0.0,>=2.32.5, but you have requests 2.32.4 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalling - 3 \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalling - 4 \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpEzoRVkZE5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33804c9-a416-495a-8992-06639bd82f4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project ID set to: platinum-bebop-474816-p0\n",
            "BigQuery Dataset to be used/created: dataset\n"
          ]
        }
      ],
      "source": [
        "# # # # # # # # # # # # #\n",
        "# Run on Restart.       #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "from google.colab import auth\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# @title Configure GCP Project Details\n",
        "PROJECT_ID = 'platinum-bebop-474816-p0' # @param {type:\"string\"}\n",
        "DATASET = 'dataset'     # @param {type:\"string\"}\n",
        "LOCATION = 'us-central1'        # Use us-central1\n",
        "\n",
        "# Set the project ID for the gcloud command-line tool and Python libraries.\n",
        "os.environ['GCLOUD_PROJECT'] = PROJECT_ID\n",
        "\n",
        "print(f\"Project ID set to: {PROJECT_ID}\")\n",
        "print(f\"BigQuery Dataset to be used/created: {DATASET}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Define your models"
      ],
      "metadata": {
        "id": "Deu8t4FmsyHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "from google.cloud import bigquery\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-005\"\n",
        "MODEL_ID=\"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "D9xGd20UASL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable aiplatform.googleapis.com --project {PROJECT_ID}\n"
      ],
      "metadata": {
        "id": "4MngC5cV0ijO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04529b53-5981-4b88-e118-a1e667a3b6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation \"operations/acat.p2-1011251704725-84d2e9df-b66c-4839-8e0b-751f971c504d\" finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLZoJDPl-Dh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug9ldg8iZE5G"
      },
      "source": [
        "## Step 1: The Prototype - Create the Source Data Table and Define Remote Model\n",
        "\n",
        "We will start by curating our knowledge base. We will create a small, focused table in our own BigQuery dataset containing just the articles for the eight planets in our solar system. This is our \"source of truth.\"\n",
        "\n",
        "We define the embedding model as a Remote Model to be able to use for vectorizing our data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONN_ID=\"demo_connection\"\n",
        "!bq mk --connection --location={LOCATION} --project_id={PROJECT_ID} \\\n",
        "  --connection_type=CLOUD_RESOURCE {CONN_ID}\n",
        "\n",
        "!bq show --connection {PROJECT_ID}.{LOCATION}.{CONN_ID}"
      ],
      "metadata": {
        "id": "Bc6hot53-HYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d63a26e-8f72-4db0-c6b7-95cf33b33f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection 1011251704725.us-central1.demo_connection successfully created\n",
            "Connection platinum-bebop-474816-p0.us-central1.demo_connection\n",
            "\n",
            "                    name                      friendlyName   description    Last modified         type        hasCredential                                             properties                                            \n",
            " ------------------------------------------- -------------- ------------- ----------------- ---------------- --------------- ------------------------------------------------------------------------------------------------ \n",
            "  1011251704725.us-central1.demo_connection                                11 Oct 16:36:26   CLOUD_RESOURCE   False           {\"serviceAccountId\": \"bqcx-1011251704725-xcx2@gcp-sa-bigquery-condel.iam.gserviceaccount.com\"}  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the service account name from above command\n",
        "SERV_ACCOUNT=\"bqcx-1011251704725-xcx2@gcp-sa-bigquery-condel.iam.gserviceaccount.com\"\n",
        "\n",
        "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "  --member=\"serviceAccount:{SERV_ACCOUNT}\" \\\n",
        "  --role=\"roles/aiplatform.user\""
      ],
      "metadata": {
        "id": "Xw2jzZEm-IRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917ca3b6-2f0f-4f74-d916-699e88e70447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated IAM policy for project [platinum-bebop-474816-p0].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:bqcx-1011251704725-xcx2@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - user:Renmsd@gmail.com\n",
            "  role: roles/owner\n",
            "etag: BwZA5KbfgZw=\n",
            "version: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0tWwydCZE5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ac3805-ab21-41b7-9c09-867448d3d0b3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'platinum-bebop-474816-p0.dataset' is ready.\n",
            "Creating curated source table from public Wikipedia data...\n",
            "Table `platinum-bebop-474816-p0.dataset.wikipedia_planets` created successfully.\n",
            "Creating remote text embedding model\n",
            "Remote model `platinum-bebop-474816-p0.dataset.text_embedding_model` created successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "dataset_id = f\"{PROJECT_ID}.{DATASET}\"\n",
        "dataset = bigquery.Dataset(dataset_id)\n",
        "dataset.location = LOCATION\n",
        "bq_client.create_dataset(dataset, exists_ok=True)\n",
        "print(f\"Dataset '{dataset_id}' is ready.\")\n",
        "\n",
        "create_source_table_sql = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{dataset_id}.wikipedia_planets` AS\n",
        "SELECT\n",
        "  page_title as title,\n",
        "  summary as text\n",
        "FROM\n",
        "  `bqaiqtest.rag_demo_dataset.wikipedia_planets_source`\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"Creating curated source table from public Wikipedia data...\")\n",
        "job = bq_client.query(create_source_table_sql)\n",
        "job.result() # Wait for the job to complete\n",
        "print(f\"Table `{dataset_id}.wikipedia_planets` created successfully.\")\n",
        "\n",
        "\n",
        "# Create the remote text embedding model to use\n",
        "\n",
        "create_remote_model_sql = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{dataset_id}.text_embedding_model`\n",
        "REMOTE WITH CONNECTION DEFAULT\n",
        "OPTIONS(ENDPOINT = '{EMBEDDING_MODEL}');\n",
        "\"\"\"\n",
        "print(\"Creating remote text embedding model\")\n",
        "\n",
        "job = bq_client.query(create_remote_model_sql)\n",
        "job.result() # Wait for the job to complete\n",
        "print(f\"Remote model `{dataset_id}.text_embedding_model` created successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGSIH3mZZE5H"
      },
      "source": [
        "## Step 2: The Prototype - Build the RAG Knowledge Base\n",
        "\n",
        "Here, we build our initial RAG store. To simulate a real-world scenario, we'll add an `access_level` column to our data, marking some documents as 'Internal' and some as 'Public'. This will be crucial for the 'Secure' step later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIUQ_9tlZE5H"
      },
      "outputs": [],
      "source": [
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "import uuid\n",
        "import random\n",
        "import pandas as pd\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from google.cloud.bigquery import ScalarQueryParameter\n",
        "from google.cloud.bigquery.job import LoadJobConfig\n",
        "from google.cloud.bigquery.table import Table\n",
        "\n",
        "\n",
        "\n",
        "def build_rag_store(rag_table_name: str, chunk_size: int, chunk_overlap: int):\n",
        "    \"\"\"Reads from the source BQ table, chunks text, creates embeddings using BigQuery, and loads to a new RAG table.\"\"\"\n",
        "\n",
        "    dataset_id = f\"{PROJECT_ID}.{DATASET}\"\n",
        "    source_table_id = f\"{dataset_id}.wikipedia_planets\"\n",
        "    destination_table_id = f\"{dataset_id}.{rag_table_name}\"\n",
        "    temp_chunked_table_id = f\"{dataset_id}.temp_chunked_{rag_table_name}\"\n",
        "\n",
        "\n",
        "    # 1. Create the destination RAG table schema with an access_level column\n",
        "    create_rag_table_sql = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{destination_table_id}` (\n",
        "      chunk_id STRING NOT NULL,\n",
        "      source_title STRING,\n",
        "      content STRING,\n",
        "      access_level STRING,\n",
        "      embedding ARRAY<FLOAT64>\n",
        "    );\n",
        "    \"\"\"\n",
        "    print(f\"Creating RAG table: {destination_table_id}\")\n",
        "    bq_client.query(create_rag_table_sql).result()\n",
        "\n",
        "    # 2. Read source data and chunk documents\n",
        "    print(f\"Reading data from {source_table_id} and chunking...\")\n",
        "    # Workaround for pyarrow/pandas compatibility issue: Fetch results as list of dicts\n",
        "    query_job = bq_client.query(f\"SELECT title, text FROM `{source_table_id}`\")\n",
        "    source_data = [dict(row) for row in query_job.result()]\n",
        "    source_df = pd.DataFrame(source_data)\n",
        "\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    rows_to_insert_temp = []\n",
        "    for _, row in source_df.iterrows():\n",
        "        chunks = text_splitter.split_text(row['text'])\n",
        "        for chunk in chunks:\n",
        "            rows_to_insert_temp.append({\n",
        "                \"chunk_id\": str(uuid.uuid4()),\n",
        "                \"source_title\": row['title'],\n",
        "                \"content\": chunk,\n",
        "                 # Simulate a mix of public and internal documents\n",
        "                \"access_level\": random.choice(['Public', 'Internal'])\n",
        "            })\n",
        "    print(f\"Generated a total of {len(rows_to_insert_temp)} chunks.\")\n",
        "\n",
        "    # 3. Load chunked data to a temporary BigQuery table\n",
        "    print(f\"Loading chunked data to a temporary BigQuery table: {temp_chunked_table_id}\")\n",
        "    # Define the schema for the temporary table\n",
        "    temp_table_schema = [\n",
        "        bigquery.SchemaField(\"chunk_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"source_title\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"content\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"access_level\", \"STRING\"),\n",
        "    ]\n",
        "    temp_table = Table(temp_chunked_table_id, schema=temp_table_schema)\n",
        "\n",
        "    # Create the temporary table, ensuring it exists before inserting\n",
        "    bq_client.create_table(temp_table, exists_ok=True)\n",
        "\n",
        "    errors = bq_client.insert_rows_json(temp_chunked_table_id, rows_to_insert_temp)\n",
        "    if errors:\n",
        "        print(f\"Encountered errors loading to temporary table: {errors}\")\n",
        "        return # Exit if loading to temp table fails\n",
        "\n",
        "    # 4. Generate embeddings using BigQuery ML\n",
        "    print(f\"Generating embeddings using BigQuery ML ({EMBEDDING_MODEL})...\")\n",
        "    generate_embeddings_sql = f\"\"\"\n",
        "    INSERT INTO `{destination_table_id}` (chunk_id, source_title, content, access_level, embedding)\n",
        "    SELECT\n",
        "        chunk_id,\n",
        "        source_title,\n",
        "        content,\n",
        "        access_level,\n",
        "        ml_generate_embedding_result\n",
        "    FROM\n",
        "        ML.GENERATE_EMBEDDING(\n",
        "            MODEL `{dataset_id}.text_embedding_model`,\n",
        "            (SELECT chunk_id, source_title, content, access_level FROM `{temp_chunked_table_id}`)\n",
        "        )\n",
        "    \"\"\"\n",
        "    # print(generate_embeddings_sql)\n",
        "    job = bq_client.query(generate_embeddings_sql)\n",
        "    job.result() # Wait for the job to complete\n",
        "    print(f\"Successfully generated embeddings and loaded to {destination_table_id}\")\n",
        "\n",
        "    # Clean up the temporary table\n",
        "    bq_client.query(f\"DROP TABLE `{temp_chunked_table_id}`\").result()\n",
        "    print(f\"Temporary table {temp_chunked_table_id} dropped.\")\n",
        "\n",
        "\n",
        "BASELINE_RAG_TABLE = \"planets_rag_baseline\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Build our first, baseline RAG store ---\n",
        "build_rag_store(\n",
        "    rag_table_name=BASELINE_RAG_TABLE,\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")"
      ],
      "metadata": {
        "id": "q_oSmmsgjD-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSBXR0buZE5H"
      },
      "source": [
        "## Step 3: The Prototype - Create a Vector Index for Fast Retrieval\n",
        "\n",
        "To make our similarity searches fast, we create a vector index. This is a crucial step for ensuring a good user experience in a production application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPlCAWHPZE5H"
      },
      "outputs": [],
      "source": [
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_vector_index(rag_table_name: str):\n",
        "    index_name = f\"{rag_table_name}_embedding_index\"\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET}.{rag_table_name}\"\n",
        "\n",
        "    sql = f\"\"\"\n",
        "    CREATE OR REPLACE VECTOR INDEX `{index_name}`\n",
        "    ON `{table_id}`(embedding)\n",
        "    OPTIONS(index_type='IVF', distance_type='COSINE');\n",
        "    \"\"\"\n",
        "    print(f\"Creating vector index `{index_name}` on table `{table_id}`. This may take a few minutes...\")\n",
        "    print(f\"If you have less than 5000 rows, an error is returned. Ignore it and go to next step\")\n",
        "\n",
        "    job = bq_client.query(sql)\n",
        "    job.result()\n",
        "    print(\"Vector index created successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the index for our baseline table\n",
        "\n",
        "\n",
        "create_vector_index(BASELINE_RAG_TABLE)"
      ],
      "metadata": {
        "id": "VeCxuhhXJqD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XAOV3V_ZE5H"
      },
      "source": [
        "## Step 4: The Last Mile - SECURE Your RAG Pipeline\n",
        "\n",
        "A prototype often ignores security, but a production system cannot. We will now upgrade our RAG pipeline to be 'security-aware'. It will take a `user_access_level` and filter the retrieved documents to ensure a user can only see 'Public' information or information matching their access level.\n",
        "\n",
        "We achieve this by adding a `filter` option to the `VECTOR_SEARCH` function, which is the most efficient way to apply a metadata filter *before* the vector search runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XGGXI6aZE5H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "\n",
        "from langchain_google_vertexai import VertexAI\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "\n",
        "GENERATION_MODEL = MODEL_ID\n",
        "llm = VertexAI(model_name=GENERATION_MODEL, project=PROJECT_ID)\n",
        "embeddings_service = VertexAIEmbeddings(model_name=EMBEDDING_MODEL, project=PROJECT_ID)\n",
        "\n",
        "def ask_rag_system(question: str, rag_table_name: str, user_access_level: str = 'Public') -> dict:\n",
        "    \"\"\"Combines SECURE retrieval and generation to answer a question.\"\"\"\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET}.{rag_table_name}\"\n",
        "\n",
        "    # 1. Generate an embedding for the user's question\n",
        "    question_embedding = embeddings_service.embed_query(question)\n",
        "\n",
        "    # 2. Retrieve relevant chunks using a SECURE VECTOR_SEARCH\n",
        "    # We filter to only include chunks that are 'Public' or match the user's access level.\n",
        "    security_filter = f\"access_level = 'Public' OR access_level = '{user_access_level}'\"\n",
        "    security_filter = f\"access_level = '{user_access_level}'\"\n",
        "    # print(security_filter)\n",
        "\n",
        "    security_filter_clause=\"WHERE \" f\"{security_filter}\"\n",
        "    retrieve_query = f\"\"\"\n",
        "    SELECT\n",
        "        base.content, base.access_level\n",
        "    FROM\n",
        "        VECTOR_SEARCH(\n",
        "             ( SELECT * FROM `{table_id}` {security_filter_clause}) ,\n",
        "            'embedding',\n",
        "            (SELECT {question_embedding} AS embedding),\n",
        "            top_k => 3,\n",
        "            distance_type => 'COSINE'\n",
        "        )\n",
        "    \"\"\"\n",
        "    # print(retrieve_query)\n",
        "    results = bq_client.query(retrieve_query).result()\n",
        "    context_chunks = [row.content for row in results]\n",
        "    context_str = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    # 3. Generate the final answer\n",
        "    prompt = f\"\"\"\n",
        "    Based ONLY on the following context, answer the user's question.\n",
        "    If the context does not contain the answer, say \"The provided context does not have the answer.\"\n",
        "\n",
        "    Context:\n",
        "    {context_str}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    answer = llm.invoke(prompt)\n",
        "\n",
        "    # The dictionary returned will be used for both logging and evaluation.\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"predicted_answer\": answer,\n",
        "        \"context\": context_str,\n",
        "        \"user_access_level\": user_access_level\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test the security feature ---\n",
        "test_question = \"Describe the atmosphere of Venus.\"\n",
        "print(\"--- Querying as a 'Public' user ---\")\n",
        "public_result = ask_rag_system(test_question, BASELINE_RAG_TABLE, user_access_level='Public')\n",
        "print(f\"Answer for Public User:\\n{public_result['predicted_answer']}\")\n",
        "\n",
        "print(\"\\n--- Querying as an 'Internal' user ---\")\n",
        "internal_result = ask_rag_system(test_question, BASELINE_RAG_TABLE, user_access_level='Internal')\n",
        "print(f\"Answer for Internal User:\\n{internal_result['predicted_answer']}\")\n",
        "print(\"\\nNote: The answers may differ if the retrieval process found different context due to access levels.\")"
      ],
      "metadata": {
        "id": "kdEKsUBGAZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i12GkvcoZE5I"
      },
      "source": [
        "## Step 5: The Last Mile - MONITOR Your RAG System\n",
        "\n",
        "A production system that isn't monitored is a black box. We need to log interactions to understand how our system is being used, where it's failing, and how to improve it.\n",
        "\n",
        "We'll create a simple logging table in BigQuery and a function to record each interaction. For this demo, we'll simulate a user feedback score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNCxxYbmZE5I"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "\n",
        "LOGGING_TABLE = f\"{PROJECT_ID}.{DATASET}.rag_monitoring_logs\"\n",
        "\n",
        "# 1. Create the logging table\n",
        "create_logging_table_sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS  `{LOGGING_TABLE}` (\n",
        "    timestamp TIMESTAMP,\n",
        "    question STRING,\n",
        "    predicted_answer STRING,\n",
        "    context STRING,\n",
        "    user_access_level STRING,\n",
        "    simulated_feedback_score INT64 -- e.g., 1 to 5\n",
        ");\n",
        "\"\"\"\n",
        "print(f\"Creating logging table: {LOGGING_TABLE}\")\n",
        "bq_client.query(create_logging_table_sql).result()\n",
        "# time.sleep(30)\n",
        "\n",
        "# 2. Create a logging function\n",
        "def log_rag_interaction(rag_result: dict):\n",
        "    \"\"\"Writes a record of the RAG interaction to a BigQuery logging table.\"\"\"\n",
        "    from datetime import datetime\n",
        "\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.utcnow().isoformat(),\n",
        "        \"question\": rag_result['question'],\n",
        "        \"predicted_answer\": rag_result['predicted_answer'],\n",
        "        \"context\": rag_result['context'],\n",
        "        \"user_access_level\": rag_result['user_access_level'],\n",
        "        \"simulated_feedback_score\": random.randint(3, 5) # Simulate good user feedback\n",
        "    }\n",
        "\n",
        "    errors = bq_client.insert_rows_json(LOGGING_TABLE, [log_entry])\n",
        "    if errors:\n",
        "        print(f\"Failed to log interaction: {errors}\")\n",
        "\n",
        "# 3. Test the logging\n",
        "print(\"Running a test query to generate a log entry...\")\n",
        "test_log_result = ask_rag_system(\"Which planet is known as the Red Planet?\", BASELINE_RAG_TABLE)\n",
        "log_rag_interaction(test_log_result)\n",
        "print(\"Log entry created.\")\n",
        "\n",
        "# 4. Query the logs to show how monitoring would work\n",
        "print(\"\\n--- Sample query on monitoring logs --- \")\n",
        "log_query_df = bq_client.query(f\"SELECT timestamp, question, simulated_feedback_score FROM `{LOGGING_TABLE}` ORDER BY timestamp DESC LIMIT 5\").to_dataframe()\n",
        "print(log_query_df)\n",
        "display(log_query_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq91Ubb1ZE5I"
      },
      "source": [
        "\n",
        "## Step 6: The Last Mile - VALIDATE RAG Performance\n",
        "\n",
        "With our secure and monitored pipeline in place, we now validate its quality. We will use the **Vertex AI Evaluation Service** to systematically measure performance.\n",
        "\n",
        "We will calculate  key metrics:\n",
        "1.  **Groundedness:** Does the generated answer stay faithful to the retrieved context? (Measures hallucinations)\n",
        "2.  **Accuracy:** Does it provide accurate information consistent with the retrieved context\n",
        "3. **Completeness:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Define the evaluation datasets.\n",
        "\n",
        "The evaluation datasets contains\n",
        " - Query/Prompt - The prompt for which we are evaluating.\n",
        " - Context - This is obtained when we run the prompt against the vector database. These are the vectors/text chunks passed to the LLM for response generation\n",
        " - Response - The respone from the LLM\n",
        " - Ground Truth - Depending on the metric, we may provide the exepected output to compare against the LLM Generated response\n",
        "\n",
        " Here we are creating two datasets: eval_dataset_small and eval_dataset_large\n",
        " You can use just one or both"
      ],
      "metadata": {
        "id": "VSC8dN4eORK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "from tqdm import tqdm\n",
        "evaluation_dataset = pd.DataFrame({\n",
        "    \"question\": [\n",
        "        \"What is the Great Red Spot on Jupiter?\",\n",
        "        \"What are Saturn's rings primarily made of?\",\n",
        "        \"Which planet is known as the Red Planet?\",\n",
        "        \"Describe the atmosphere of Venus.\"\n",
        "    ],\n",
        "    \"ground_truth_answer\": [\n",
        "        \"The Great Red Spot is a persistent high-pressure region in the atmosphere of Jupiter, producing an anticyclonic storm that is the largest in the Solar System.\",\n",
        "        \"The rings consist of countless small particles, ranging in size from micrometers to meters, that orbit Saturn. The particles are made almost entirely of water ice, with a trace component of rocky material.\",\n",
        "        \"Mars is known as the Red Planet due to the prevalence of iron oxide on its surface, which gives it a reddish appearance.\",\n",
        "        \"Venus has an extremely dense atmosphere composed of more than 96% carbon dioxide, with the rest being nitrogen and trace gases. The atmosphere traps heat, creating a runaway greenhouse effect resulting in surface temperatures of around 462 °C (864 °F).\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "\n",
        "rag_retrieved_context = []\n",
        "rag_generated_response = []\n",
        "rag_prompt=[]\n",
        "\n",
        "for prompt in tqdm(evaluation_dataset['question']):\n",
        "\n",
        "   rag_result = ask_rag_system(prompt, BASELINE_RAG_TABLE, user_access_level='Public')\n",
        "   rag_prompt.append(rag_result['question'])\n",
        "   rag_retrieved_context.append(rag_result['context'])\n",
        "   rag_generated_response.append(rag_result['predicted_answer'])\n",
        "\n",
        "\n",
        "eval_dataset_small = pd.DataFrame(\n",
        "    {\n",
        "        \"prompt\": rag_prompt,\n",
        "        \"retrieved_context\": rag_retrieved_context,\n",
        "        \"response\": rag_generated_response,\n",
        "    }\n",
        ")\n",
        "\n",
        "eval_dataset_small\n",
        "\n"
      ],
      "metadata": {
        "id": "1JCFVhwn3Glk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "\n",
        "#  This creates a larger dataset for evaluation. (optional)\n",
        "\n",
        "questions =  [\n",
        "\n",
        "\n",
        "\"Which planet is known as the Red Planet?\",\n",
        "\n",
        "\"What are the rings of Saturn primarily made of?\",\n",
        "\n",
        "\"How long is a year on Mercury in Earth days?\",\n",
        "\n",
        "\"Which planet is the smallest in our solar system?\",\n",
        "\n",
        "\"What is the name of the largest volcano on Mars?\",\n",
        "\n",
        "\"Which planet is the windiest in the solar system?\",\n",
        "\n",
        "\"What is the main component of Venus's atmosphere?\",\n",
        "\n",
        "\"Who is the planet Neptune named after?\",\n",
        "\n",
        "\n",
        "\n",
        "\"Describe the Great Red Spot on Jupiter.\",\n",
        "\n",
        "\"Explain the greenhouse effect that occurs on Venus.\",\n",
        "\n",
        "\"Summarize the key characteristics of Earth's magnetosphere.\",\n",
        "\n",
        "\"What is the composition of Jupiter's atmosphere?\",\n",
        "\n",
        "\"Describe the physical appearance and color of Uranus.\",\n",
        "\n",
        "\"Explain why Mercury experiences such extreme temperature fluctuations.\",\n",
        "\n",
        "\"What does the article say about the axial tilt of Uranus?\",\n",
        "\n",
        "\n",
        "\n",
        "\"Compare the atmospheric composition of Earth and Mars.\",\n",
        "\n",
        "\"Which planet is larger in diameter, Uranus or Neptune?\",\n",
        "\n",
        "\"Contrast the surface conditions of Venus and Mercury.\",\n",
        "\n",
        "\"How does the length of a day on Jupiter compare to a day on Earth?\",\n",
        "\n",
        "\"Which of the two ice giants, Uranus or Neptune, has a stronger magnetic field according to the texts?\",\n",
        "\n",
        "\"Compare the number of known moons for Saturn and Jupiter.\",\n",
        "\n",
        "\"Which of the terrestrial planets has the thinnest atmosphere?\",\n",
        "\n",
        "\n",
        "\n",
        "\"Does the article on Venus mention the presence of volcanoes?\",\n",
        "\n",
        "\"Is there any mention of liquid water currently on the surface of Mars in the provided text?\",\n",
        "\n",
        "\"Do the articles mention any planned future missions to Uranus?\",\n",
        " \"What is the colour of my car?\"\n",
        "  ]\n",
        "\n",
        "rag_retrieved_context = []\n",
        "rag_generated_response = []\n",
        "rag_prompt=[]\n",
        "\n",
        "for prompt in tqdm(questions):\n",
        "\n",
        "   rag_result = ask_rag_system(prompt, BASELINE_RAG_TABLE, user_access_level='Public')\n",
        "   rag_prompt.append(rag_result['question'])\n",
        "   rag_retrieved_context.append(rag_result['context'])\n",
        "   rag_generated_response.append(rag_result['predicted_answer'])\n",
        "\n",
        "\n",
        "eval_dataset_large = pd.DataFrame(\n",
        "    {\n",
        "        \"prompt\": rag_prompt,\n",
        "        \"retrieved_context\": rag_retrieved_context,\n",
        "        \"response\": rag_generated_response,\n",
        "    }\n",
        ")\n",
        "\n",
        "eval_dataset_large[0:4]\n"
      ],
      "metadata": {
        "id": "lqnAB3snSthC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # #\n",
        "# Run on Restart. #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "from vertexai.evaluation import (\n",
        "    EvalTask,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        "    notebook_utils,\n",
        ")\n",
        "\n",
        "custom_question_answering_correctness = PointwiseMetric(\n",
        "    metric=\"custom_question_answering_correctness\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "            \"accuracy\": (\n",
        "                \"The response provides completely accurate information, consistent with the retrieved context, with no errors or omissions.\"\n",
        "            ),\n",
        "            \"completeness\": (\n",
        "                \"The response answers all parts of the question fully, utilizing the information available in the retrieved context.\"\n",
        "            ),\n",
        "            \"groundedness\": (\n",
        "                \"The response uses only the information provided in the retrieved context and does not introduce any external information or hallucinations.\"\n",
        "            ),\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"5\": \"(Very good). The answer is completely accurate, complete, concise, grounded in the retrieved context, and follows all instructions.\",\n",
        "            \"4\": \"(Good). The answer is mostly accurate, complete, and grounded in the retrieved context, with minor issues in conciseness or instruction following.\",\n",
        "            \"3\": \"(Ok). The answer is partially accurate and complete but may have some inaccuracies, omissions, or significant issues with conciseness, groundedness, or instruction following, based on the retrieved context.\",\n",
        "            \"2\": \"(Bad). The answer contains significant inaccuracies, is largely incomplete, or fails to follow key instructions, considering the information available in the retrieved context.\",\n",
        "            \"1\": \"(Very bad). The answer is completely inaccurate, irrelevant, or fails to address the question in any meaningful way, based on the retrieved context.\",\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"retrieved_context\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "accuracy_metric = PointwiseMetric(\n",
        "    metric=\"accuracy\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "            \"accuracy\": (\n",
        "                \"The response provides completely accurate information, consistent with the retrieved context, with no errors or omissions.\"\n",
        "            )\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"5\": \"(Very good). The answer is completely accuratein the retrieved context, and follows all instructions.\",\n",
        "            \"4\": \"(Good). The answer is mostly accurate in the retrieved context, with minor issues in conciseness or instruction following.\",\n",
        "            \"3\": \"(Ok). The answer is partially accurate based on the retrieved context\",\n",
        "            \"2\": \"(Bad). The answer contains significant inaccuracies\",\n",
        "            \"1\": \"(Very bad). The answer is completely inaccurate, irrelevant, or fails to address the question in any meaningful way, based on the retrieved context.\",\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"retrieved_context\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "completeness_metric = PointwiseMetric(\n",
        "    metric=\"completeness\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "              \"completeness\": (\n",
        "                \"The response answers all parts of the question fully, utilizing the information available in the retrieved context.\"\n",
        "            ),\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"5\": \"(Very good). The answer is  complete, concise and follows all instructions.\",\n",
        "            \"4\": \"(Good). The answer is mostly complete with minor issues in conciseness or instruction following.\",\n",
        "            \"3\": \"(Ok). The answer is partially  complete based on the retrieved context.\",\n",
        "            \"2\": \"(Bad). The answer  is largely incomplete, or fails to follow key instructions, considering the information available in the retrieved context.\",\n",
        "            \"1\": \"(Very bad). The answer fails to address the question in any meaningful way, based on the retrieved context.\",\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"retrieved_context\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "groundedness_metric = PointwiseMetric(\n",
        "    metric=\"groundedness\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "\n",
        "            \"groundedness\": (\n",
        "                \"The response uses only the information provided in the retrieved context and does not introduce any external information or hallucinations.\"\n",
        "            ),\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"5\": \"(Very good). The answer is completely grounded in the retrieved context, and follows all instructions.\",\n",
        "            \"4\": \"(Good). The answer is mostly grounded in the retrieved context, with minor issues in conciseness or instruction following.\",\n",
        "            \"3\": \"(Ok). The answer is partially grounded  based on the retrieved context.\",\n",
        "            \"2\": \"(Bad). The answer contains significant differences considering the information available in the retrieved context.\",\n",
        "            \"1\": \"(Very bad). The answer fails to address the question in any meaningful way, based on the retrieved context.\",\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"retrieved_context\"],\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Display the serialized metric prompt template\n",
        "# print(custom_question_answering_correctness.metric_prompt_template)\n",
        "\n",
        "print(completeness_metric.metric_prompt_template)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z7ERoboUN7Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # # # # # # # # # # #\n",
        "# Run on Restart.       #\n",
        "# # # # # # # # # # # #\n",
        "\n",
        "def run_evaluation(evaluation_ds, metrics_list, experiment_name):\n",
        "  eval_task = EvalTask(\n",
        "      dataset=evaluation_ds,\n",
        "      metrics=metrics_list,\n",
        "      experiment=experiment_name,\n",
        "  )\n",
        "  return eval_task.evaluate()\n",
        "\n",
        "\n",
        "def print_eval_results(eval_result, metrics_list):\n",
        "\n",
        "  notebook_utils.display_eval_result(eval_result=eval_result)\n",
        "\n",
        "  notebook_utils.display_radar_plot(\n",
        "            eval_results_with_title=[(\"Question answering correctness\", eval_result)],\n",
        "            metrics = metrics_list\n",
        "  )\n",
        "\n",
        "  notebook_utils.display_bar_plot(\n",
        "            eval_results_with_title=[(\"Question answering correctness\", eval_result)],\n",
        "            metrics=metrics_list,\n",
        "        )\n",
        "\n",
        "  notebook_utils.display_explanations(eval_result=eval_result, num=1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6zZc22G6OrHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "base_eval_result_small_ds=run_evaluation(eval_dataset_small, [accuracy_metric, completeness_metric, groundedness_metric], \"base-evaluation-small-ds\")\n",
        "\n",
        "print_eval_results(base_eval_result_small_ds, [\"accuracy\",\"completeness\",\"groundedness\"] )\n",
        "\n",
        "base_eval_small_ds_summary=base_eval_result_small_ds.summary_metrics\n"
      ],
      "metadata": {
        "id": "7GVs3ed1X84x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_eval_result_large_ds=run_evaluation(eval_dataset_large, [accuracy_metric, completeness_metric, groundedness_metric], \"base-evaluation-large-ds\")\n",
        "\n",
        "print_eval_results(base_eval_result_large_ds, [\"accuracy\",\"completeness\",\"groundedness\"] )\n",
        "\n",
        "base_eval_large_ds_summary=base_eval_result_large_ds.summary_metrics\n"
      ],
      "metadata": {
        "id": "_QCyJBqZZXZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KF3ebFNMa3g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abCXlU6aO9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yu4zLmegRMfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjYtHNDwiLNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWNFnuhSZE5I"
      },
      "source": [
        "## Step 7: Closing the Loop - TUNE and Re-Evaluate\n",
        "\n",
        "Our baseline system has a set of scores. Can we do better? Let's hypothesize that a smaller chunk size might provide more focused context to the LLM and improve performance.\n",
        "\n",
        "This is the core loop of production RAG development: **Validate -> Tune -> Re-Validate**."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1v9zwPOImKxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8bh2NHYZE5I"
      },
      "outputs": [],
      "source": [
        "# --- 1. Build the TUNED RAG store with smaller chunks ---\n",
        "TUNED_RAG_TABLE = \"planets_rag_tuned\"\n",
        "\n",
        "build_rag_store(\n",
        "    rag_table_name=TUNED_RAG_TABLE,\n",
        "    chunk_size=250, # Drastically smaller chunk size\n",
        "    chunk_overlap=25\n",
        ")\n",
        "\n",
        "# --- 2. Create a vector index for the new table ---  (Run if table is over 5000 rows)\n",
        "# create_vector_index(TUNED_RAG_TABLE)\n",
        "\n",
        "# --- 3. Evaluate the TUNED model ---\n",
        "print(\"\\nRunning evaluation for the TUNED RAG system (chunk_size=250)...\")\n",
        "tuned_eval_result_large_ds=run_evaluation(eval_dataset_large, [accuracy_metric, completeness_metric, groundedness_metric], \"tuned-evaluation-large-ds\")\n",
        "# print_eval_results(tuned_eval_result_large_ds, [\"accuracy\",\"completeness\",\"groundedness\"] )\n",
        "tuned_eval_large_ds_summary=tuned_eval_result_large_ds.summary_metrics\n",
        "\n",
        "tuned_eval_result_small_ds=run_evaluation(eval_dataset_small, [accuracy_metric, completeness_metric, groundedness_metric], \"tuned-evaluation-small-ds\")\n",
        "# print_eval_results(tuned_eval_result_small_ds, [\"accuracy\",\"completeness\",\"groundedness\"] )\n",
        "tuned_eval_small_ds_summary=tuned_eval_result_small_ds.summary_metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVj4LcBwZE5I"
      },
      "source": [
        "## Step 8: Final Analysis - Conquering the Last Mile\n",
        "\n",
        "Now we compare the metrics from our two experiments. This is the payoff—we have concrete data to support our tuning decisions, turning guesswork into engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmkZkjPQZE5J"
      },
      "outputs": [],
      "source": [
        "print(\"--- Final Comparison of Large Dataset---\")\n",
        "print(f\"Baseline (chunk_size=1000): Accuracy={base_eval_large_ds_summary['accuracy/mean']:.4f}, Completeness={base_eval_large_ds_summary['completeness/mean']:.4f}, Groundedness={base_eval_large_ds_summary['groundedness/mean']:.4f}\")\n",
        "print(f\"Tuned    (chunk_size=250): Accuracy={tuned_eval_large_ds_summary['accuracy/mean']:.4f}, Completeness={tuned_eval_large_ds_summary['completeness/mean']:.4f}, , Groundedness={tuned_eval_large_ds_summary['groundedness/mean']:.4f}\")\n",
        "print(\"------------------------\")\n",
        "\n",
        "notebook_utils.display_bar_plot(\n",
        "    eval_results_with_title=[\n",
        "        (\"Baseline (chunk_size=1000)\", base_eval_result_large_ds),\n",
        "        (\"Tuned (chunk_size=250)\", tuned_eval_result_large_ds),\n",
        "    ],\n",
        "    metrics=[\"accuracy\", \"completeness\", \"groundedness\"],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nBy following the 'Validate, Secure, Monitor' framework, we have successfully navigated the 'last mile'.\")\n",
        "print(\"We've moved beyond a simple prototype to a system that is measurable, secure, and ready for production challenges.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additioanl Evaluation : RAGAS Metrics\n",
        "\n",
        "Use RAGAS framework to evaluate the datasets\n",
        "\n",
        "\n",
        "(a) **Context Precision:** proportion of retrieved contexts that are relevant to the query\n",
        "\n",
        "(b) **Faithfulness:** Same as accuracy\n",
        "\n",
        "(c) **Context Recall:** Quantifies the extent to which the relevant information\n",
        "\n",
        "(d) **Context Relevance:** Evaluates whether the retrieved_contexts are pertinent to the user_input.\n",
        "\n",
        "(e) **ROUGE Score:** evaluate the quality of natural language generations. It measures the overlap between the generated response and the reference text"
      ],
      "metadata": {
        "id": "yk5Puno8ye8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to display eval results\n"
      ],
      "metadata": {
        "id": "dLRZsW_e1GgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Mismatch between the VertexAI library RAGAS uses and what we have used above.\n",
        "#  So, new definitions needed\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import HTML, Markdown, display\n",
        "\n",
        "\n",
        "def display_eval_report(eval_result, metrics=None):\n",
        "    \"\"\"Display the evaluation results.\"\"\"\n",
        "\n",
        "    # title, summary_metrics, report_df = eval_result\n",
        "    title=\"Report\"\n",
        "    summary_metrics=eval_result.summary_metrics\n",
        "    report_df=eval_result.metrics_table\n",
        "\n",
        "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
        "    if metrics:\n",
        "        metrics_df = metrics_df.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in metrics_df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "        report_df = report_df.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in report_df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Display the title with Markdown for emphasis\n",
        "    display(Markdown(f\"## {title}\"))\n",
        "\n",
        "    # Display the metrics DataFrame\n",
        "    display(Markdown(\"### Summary Metrics\"))\n",
        "    display(metrics_df)\n",
        "\n",
        "    # Display the detailed report DataFrame\n",
        "    display(Markdown(\"### Report Metrics\"))\n",
        "    display(report_df)\n",
        "\n",
        "\n",
        "def plot_radar_plot(eval_results, max_score=5, metrics=None):\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for eval_result in eval_results:\n",
        "        title=\"Report\"\n",
        "        summary_metrics=eval_result.summary_metrics\n",
        "        report_df=eval_result.metrics_table\n",
        "\n",
        "        if metrics:\n",
        "            summary_metrics = {\n",
        "                k: summary_metrics[k]\n",
        "                for k, v in summary_metrics.items()\n",
        "                if any(selected_metric in k for selected_metric in metrics)\n",
        "            }\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(\n",
        "                r=list(summary_metrics.values()),\n",
        "                theta=list(summary_metrics.keys()),\n",
        "                fill=\"toself\",\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        polar=dict(radialaxis=dict(visible=True, range=[0, max_score])), showlegend=True\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def plot_bar_plot(eval_results, metrics=None):\n",
        "    fig = go.Figure()\n",
        "    data = []\n",
        "\n",
        "    for eval_result in eval_results:\n",
        "\n",
        "        title=\"Report\"\n",
        "        summary_metrics=eval_result.summary_metrics\n",
        "        report_df=eval_result.metrics_table\n",
        "\n",
        "\n",
        "        if metrics:\n",
        "            summary_metrics = {\n",
        "                k: summary_metrics[k]\n",
        "                for k, v in summary_metrics.items()\n",
        "                if any(selected_metric in k for selected_metric in metrics)\n",
        "            }\n",
        "\n",
        "        data.append(\n",
        "            go.Bar(\n",
        "                x=list(summary_metrics.keys()),\n",
        "                y=list(summary_metrics.values()),\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    # Change the bar mode\n",
        "    fig.update_layout(barmode=\"group\")\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "mcSTpVeWgOyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize and define metrics\n"
      ],
      "metadata": {
        "id": "B3MqAP3U1XIG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XESQ92nt1Vdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
        "\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(VertexAI(model_name=MODEL_ID))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(VertexAIEmbeddings(model_name=EMBEDDING_MODEL))"
      ],
      "metadata": {
        "id": "A1AejFr5gYXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import ContextPrecision, Faithfulness, RubricsScore, RougeScore, ContextRecall, ContextRelevance\n",
        "from ragas.metrics import RougeScore\n",
        "rouge_score = RougeScore()\n",
        "\n",
        "helpfulness_rubrics = {\n",
        "    \"score1_description\": \"Response is useless/irrelevant, contains inaccurate/deceptive/misleading information, and/or contains harmful/offensive content. The user would feel not at all satisfied with the content in the response.\",\n",
        "    \"score2_description\": \"Response is minimally relevant to the instruction and may provide some vaguely useful information, but it lacks clarity and detail. It might contain minor inaccuracies. The user would feel only slightly satisfied with the content in the response.\",\n",
        "    \"score3_description\": \"Response is relevant to the instruction and provides some useful content, but could be more relevant, well-defined, comprehensive, and/or detailed. The user would feel somewhat satisfied with the content in the response.\",\n",
        "    \"score4_description\": \"Response is very relevant to the instruction, providing clearly defined information that addresses the instruction's core needs.  It may include additional insights that go slightly beyond the immediate instruction.  The user would feel quite satisfied with the content in the response.\",\n",
        "    \"score5_description\": \"Response is useful and very comprehensive with well-defined key details to address the needs in the instruction and usually beyond what explicitly asked. The user would feel very satisfied with the content in the response.\",\n",
        "}\n",
        "\n",
        "rubrics_score = RubricsScore(name=\"helpfulness\", rubrics=helpfulness_rubrics)\n",
        "context_precision = ContextPrecision(llm=evaluator_llm)\n",
        "faithfulness = Faithfulness(llm=evaluator_llm)\n",
        "context_recall=ContextRecall(llm=evaluator_llm)\n",
        "context_relevance=ContextRelevance(llm=evaluator_llm)\n",
        "rouge_score = RougeScore()"
      ],
      "metadata": {
        "id": "G2t4AyCMgtS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare evaluation dataset in RAGAS format\n"
      ],
      "metadata": {
        "id": "ddW1SITt1j9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
        "\n",
        "n = len(eval_dataset_small)\n",
        "\n",
        "samples_a = []\n",
        "\n",
        "eval_dataset_small_prompt=eval_dataset_small['prompt'].tolist()\n",
        "eval_dataset_small_context=eval_dataset_small['retrieved_context'].tolist()\n",
        "eval_dataset_small_response=eval_dataset_small['response'].tolist()\n",
        "\n",
        "eval_dataset_small_reference=['a massive, persistent anticyclonic storm','ice particles along with some rocky debris ','Mars','sulphuric acid']\n",
        "\n",
        "\n",
        "for i in range(n):\n",
        "    sample_a = SingleTurnSample(\n",
        "        user_input=eval_dataset_small_prompt[i],\n",
        "        retrieved_contexts=[eval_dataset_small_context[i]],\n",
        "        response=eval_dataset_small_response[i],\n",
        "        reference=eval_dataset_small_reference[i]\n",
        "    )\n",
        "\n",
        "\n",
        "    samples_a.append(sample_a)\n",
        "\n",
        "ragas_eval_dataset_a = EvaluationDataset(samples=samples_a)\n",
        "ragas_eval_dataset_a.to_pandas()\n",
        "\n"
      ],
      "metadata": {
        "id": "y1MKC_Q_hyam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Evaluate*\n"
      ],
      "metadata": {
        "id": "Fi262S-L1io7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "ragas_metrics = [\n",
        "    context_precision,\n",
        "    faithfulness,\n",
        "    rouge_score,\n",
        "    rubrics_score,\n",
        "    context_recall,\n",
        "    context_relevance\n",
        "    ]\n",
        "\n",
        "ragas_result_rag_a = evaluate(\n",
        "    dataset=ragas_eval_dataset_a, metrics=ragas_metrics, llm=evaluator_llm\n",
        ")"
      ],
      "metadata": {
        "id": "VI4axVndksd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.evaluation import EvalResult\n",
        "\n",
        "result_rag_a = EvalResult(\n",
        "    summary_metrics=ragas_result_rag_a._repr_dict,\n",
        "    metrics_table=ragas_result_rag_a.to_pandas(),\n",
        ")\n",
        "\n",
        "result_rag_a.summary_metrics"
      ],
      "metadata": {
        "id": "mXB3OB-7mBAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Results\n"
      ],
      "metadata": {
        "id": "jf27TSY811NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics_list=[\"context_precision\",\"faithfulness\",\"rouge_score\",\"helpfulness\",\"context_recall\",\"context_relevance\"]\n",
        "display_eval_report(result_rag_a, metrics=metrics_list)\n",
        "plot_radar_plot([result_rag_a], metrics=metrics_list)\n",
        "plot_bar_plot([result_rag_a], metrics=metrics_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jBk-ohTopDSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}