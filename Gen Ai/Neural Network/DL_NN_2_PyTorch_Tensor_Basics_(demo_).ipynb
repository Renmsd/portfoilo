{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renmsd/portfoilo/blob/main/Gen%20Ai/Neural%20Network/DL_NN_2_PyTorch_Tensor_Basics_(demo_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10978eb0",
      "metadata": {
        "id": "10978eb0"
      },
      "source": [
        "<img src=\"https://s3.amazonaws.com/weclouddata/images/logos/wcd_logo_new_2.png\" width=\"10%\">\n",
        "<h1><center>Pytorch Basics</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d61c2f5",
      "metadata": {
        "id": "9d61c2f5"
      },
      "source": [
        "## Tensor Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4723f7a1",
      "metadata": {
        "id": "4723f7a1"
      },
      "source": [
        "### Tensor and dimension of tensor\n",
        "\n",
        "Tensor is thought of generalization of scalars, vectors, and matrices to higher dimensions.\n",
        "\n",
        "**0-D tensor**: A scalar (just a single number), for example: `1`\n",
        "\n",
        "**1-D tensor**: A vector (like a list of numbers), for example: `[1, 2, 3]`\n",
        "\n",
        "**2-D tensor**: A matrix (like a 2D grid of numbers), for example:\n",
        "```\n",
        "[\n",
        " [1,2,3],\n",
        " [4,5,6]\n",
        "]\n",
        "```\n",
        "\n",
        "**3-D tensor and higher**: Higher-dimensional arrays or tensors. Here is an exmaple for 3-D tensor:\n",
        "```\n",
        "[\n",
        " [\n",
        "  [1,2,3],\n",
        "  [4,5,6]\n",
        " ],\n",
        " [\n",
        "  [9,8,7],\n",
        "  [6,5,4]\n",
        " ]\n",
        "]\n",
        "```\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*pgVlfgFbk3t3FWe3QEVlXA.png\" alt=\"Markdown Monster icon\" width=\"30%\" />\n",
        "\n",
        "More visually:\n",
        "\n",
        "0-D tensor (Scalar) is like a single point.\n",
        "\n",
        "1-D tensor (Vector) is like a line with multiple points.\n",
        "\n",
        "2-D tensor (Matrix) is like a grid or a table. Imagine a spreadsheet or a checkerboard.\n",
        "\n",
        "3-D tensor is like a cube or box.\n",
        "\n",
        "4-D tensor is like a series of cubes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf0276c",
      "metadata": {
        "id": "2bf0276c"
      },
      "source": [
        "### Tensor Dimensions in NLP and Computer Vision Tasks\n",
        "\n",
        "Here are some sample use cases for multiple dimension tensor in NLP(Natural Language Processing) and CV(Computer Vision)\n",
        "\n",
        "<br />\n",
        "\n",
        "#### NLP\n",
        "**1. Word Embeddings**:\n",
        " - Single Word:\n",
        "   - 1-D tensor(vector): If you embed a single word using, for instance, a 300-dimensional embedding, you get a vector of size **`[embedding_dimension]`**.\n",
        "     - Example: \"cat\" → `[0.12, -0.25, ... , 0.45]` (assuming 300 elements in total)\n",
        " - Squence of words:\n",
        "   - 2-D tensors(matrix): If you have a sequence of words and represent each word by an embedding vector, the data can be represented as a matrix of size **`[sequence_length, embedding_dimension]`**.\n",
        "\n",
        "<br />\n",
        "\n",
        "**2. Batch of Sequences**:\n",
        "\n",
        " - 3-D tensors: When processing multiple sequences in a batch, it's common to have a tensor of shape **`[batch_size, sequence_length, embedding_dimension]`**.\n",
        "\n",
        " <br />\n",
        "\n",
        "**3. Recurrent Neural Networks (RNNs)**:\n",
        "\n",
        " - 3-D tensors: RNNs often operate on 3-D tensors where dimensions can represent **`[batch_size, sequence_length, features]`**.\n",
        "\n",
        " <br />\n",
        "\n",
        "**4. Transformer Architectures (like BERT)**:\n",
        "\n",
        " - 3-D tensors: Similar to RNNs, the input and intermediate representations are often 3-D tensors, e.g., **`[batch_size, sequence_length, embedding_dimension]`**.\n",
        "\n",
        " <br />\n",
        "\n",
        "**5. One-Hot Encoded Text**:\n",
        "\n",
        " - One-hot encoding represents words (or characters) as vectors where all positions are zero, except for the position corresponding to the word, which is set to 1.\n",
        "\n",
        " - Single Word:\n",
        "   - Vector (1-D tensor): For a vocabulary of size `V`, a word's one-hot representation would be a vector of size `[V]`, with one element set to 1, and all others set to 0.\n",
        "     - Example: For a vocabulary `{\"the\", \"black\", \"cat\"}`, the word \"cat\" might be represented as `[0, 0, 1]`.\n",
        " - Sequence of Words:\n",
        "   - Matrix (2-D tensor): For a sequence of n words, you'd stack the one-hot vectors, leading to a size of **`[sequence_length, V]`**.\n",
        "     - Example: \"The black cat\" → matrix of size `(3, 3)`, with each row being the one-hot representation of a word.\n",
        "     \n",
        "<br />\n",
        "\n",
        "**6. Document Embeddings**:\n",
        " - Single Document:\n",
        "   - Vector (1-D tensor): A document embedded into a fixed-size vector, e.g., 500-dimensional, would have size `[embedding_dimension]`.\n",
        "     - Example: The entire text of \"The quick brown fox jumps over the lazy dog\" is transformed into a vector like `[0.45, -0.12, ... , 0.58]` (assuming 500 elements).\n",
        " - Batch of Documents:\n",
        "   - Matrix (2-D tensor): For a batch of `m` documents, where each document is represented as a fixed-size vector (e.g., 500-dimensional), you'd have a matrix of size **`[batch_size, embedding_dimension]`**.\n",
        "     - Example: If you're processing 10 documents at once, each with a 500-dimensional embedding, the tensor shape would be `(10, 500)`.\n",
        "     \n",
        "<br />\n",
        "\n",
        "#### CV\n",
        "**1. Images**:\n",
        " - 3-D tensors: Common for individual images. Dimensions usually represent **`[height, width, channels]`** where channels are often RGB (3 channels). For grayscale images, the dimension might be just **`[height, width, 1]`**.\n",
        " - In Pytorch, the shape is `[channels, height, width]`\n",
        "\n",
        "<br />\n",
        "\n",
        "**2.Batches of Images**:\n",
        " - 4-D tensors: When processing multiple images in a batch, another dimension is added. For example, **`[batch_size, height, width, channels]`**.\n",
        " - In Pytorch, the shape is `[batch_size, channels, height, width]`\n",
        "\n",
        "<br />\n",
        "\n",
        "**3.Convolutional Neural Networks**:\n",
        "- 4-D tensors: Convolutional layers in deep learning models often operate on 4-D tensors, especially when using mini-batch training. Intermediate layers can produce tensors of varying spatial dimensions depending on the layer's operations (e.g., convolution, pooling)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae456b64",
      "metadata": {
        "id": "ae456b64"
      },
      "source": [
        "### Create Tensor in Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37da6a9",
      "metadata": {
        "id": "b37da6a9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c1ce78",
      "metadata": {
        "id": "b1c1ce78"
      },
      "source": [
        "#### Create from Numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a1a889",
      "metadata": {
        "id": "30a1a889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5595934-0711-4516-9b4d-96d10e6da382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n",
            "[1 2 3 4 5 6]\n"
          ]
        }
      ],
      "source": [
        "# create a numpy array\n",
        "num = [1,2,3,4,5,6]\n",
        "arr = np.array(num)\n",
        "\n",
        "print(num)\n",
        "print(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62386fe3",
      "metadata": {
        "id": "62386fe3"
      },
      "outputs": [],
      "source": [
        "# check the data type and arr type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1276ad",
      "metadata": {
        "id": "ad1276ad"
      },
      "outputs": [],
      "source": [
        "# convert numpy array to tensor\n",
        "my_tensor = torch.from_numpy(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243be2ca",
      "metadata": {
        "id": "243be2ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ae4a7a-e555-4d70-cadc-42c4d93d9c14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#type(my_tensor)\n",
        "type(my_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408809cf",
      "metadata": {
        "id": "408809cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402e205e-aabd-49c2-86ed-4806d7e58131"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.],\n",
              "        [ 3.,  4.,  5.],\n",
              "        [ 6.,  7.,  8.],\n",
              "        [ 9., 10., 11.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# convert 2d array to tensor\n",
        "arr2d = np.arange(0.0, 12.0)\n",
        "arr2d = arr2d.reshape(4, 3)\n",
        "arr2d\n",
        "# print(torch.tensor(arr2d))\n",
        "torch.from_numpy(arr2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf216da",
      "metadata": {
        "id": "dbf216da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c0a428-164f-46f0-ad4f-a75a4cae8e1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Create a 1D array with values from 0.0 up to (but not including) 9.0\n",
        "# Display the array\n",
        "arr = np.arange(0.0, 9.0,1)\n",
        "# torch.from_numpy(arrz)\n",
        "torch.tensor(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9efd51a",
      "metadata": {
        "id": "a9efd51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3697021e-bb78-407c-d53c-486c23d36682"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.],\n",
              "       [6., 7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Reshape the 1D array into a 3x3 2D array (matrix)\n",
        "# Display the reshaped array\n",
        "arr2d = arrz.reshape(3, 3)\n",
        "arr2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2a5a99",
      "metadata": {
        "id": "bf2a5a99"
      },
      "outputs": [],
      "source": [
        "# Convert the NumPy 2D array into a PyTorch tensor\n",
        "\n",
        "# Display the created tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b3189b",
      "metadata": {
        "id": "90b3189b"
      },
      "outputs": [],
      "source": [
        "# another way to convert the numpy array to tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bccf48a",
      "metadata": {
        "id": "0bccf48a"
      },
      "source": [
        "The different between `torch.from_numpy` and `torch.as_tensor` is that:\n",
        " - Data Type Support\n",
        "   - `torch.from_numpy` only accepts numpy arrays.\n",
        "   - `torch.as_tensor` is more versatile. It can accept a wider range of data types, including numpy arrays, Python lists, scalar values, and other tensor-like objects.\n",
        " -  Dtype and Device Handling\n",
        "    - `torch.from_numpy` does not allow you to specify the dtype or device; the returned tensor's properties are inferred from the numpy array.\n",
        "    - `torch.as_tensor` provides more flexibility. You can use the dtype and device arguments to specify the desired data type and device (e.g., CPU or GPU) for the created tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e67b853",
      "metadata": {
        "id": "5e67b853"
      },
      "outputs": [],
      "source": [
        "# The code below will give you the TypeError: expected np.ndarray (got list)\n",
        "# torch.from_numpy([1, 2, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162d9f69",
      "metadata": {
        "id": "162d9f69"
      },
      "outputs": [],
      "source": [
        "# But it works with torch.as_tensor, since it support more data types\n",
        "#torch.as_tensor([1, 2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b57f03c",
      "metadata": {
        "id": "7b57f03c"
      },
      "source": [
        "from the document of `torch.from_numpy`, we can see it only take one parameter, which is `ndarray`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38895a4",
      "metadata": {
        "scrolled": true,
        "id": "f38895a4"
      },
      "outputs": [],
      "source": [
        "help(torch.from_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d70977b",
      "metadata": {
        "id": "0d70977b"
      },
      "source": [
        "But in the document of `torch.as_tensor`, it also takes dtype and device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfea3523",
      "metadata": {
        "scrolled": true,
        "id": "bfea3523"
      },
      "outputs": [],
      "source": [
        "help(torch.as_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75b3246",
      "metadata": {
        "id": "f75b3246"
      },
      "outputs": [],
      "source": [
        "arr.dtype"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    my_tensor3 =torch.as_tensor(arrz, device=\"cuda :0\")\n",
        "    print(my_tensor3)"
      ],
      "metadata": {
        "id": "5blmH_y_byOO"
      },
      "id": "5blmH_y_byOO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cbcaade",
      "metadata": {
        "id": "3cbcaade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e34aa1-0680-46a5-e563-8cde4c6a4b09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# my_tensor2 are the tensor directly convert from the numpy arrau in our previous code\n",
        "\n",
        "#your code\n",
        "my_tensor2 = torch.as_tensor(arrz)\n",
        "my_tensor2\n",
        "# arrz = np.arange(0.0, 9.0,1)\n",
        "# # torch.from_numpy(arrz)\n",
        "# torch.tensor(arrz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91653f61",
      "metadata": {
        "id": "91653f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc476b52-219b-495e-ce5d-be699f37fe3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "# Create a tensor from NumPy array with integer data type (int64)\n",
        "my_tensor2.type(torch.int64)\n",
        "# Create another tensor from the same array with float data type (float64)\n",
        "\n",
        "# Print the integer tensor\n",
        "print(my_tensor2.dtype)\n",
        "# Print the data type of the integer tensor\n",
        "\n",
        "# Print the float tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e105fbc6",
      "metadata": {
        "id": "e105fbc6"
      },
      "outputs": [],
      "source": [
        "# use gpu with torch.as_tensor\n",
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2eaff8",
      "metadata": {
        "id": "af2eaff8"
      },
      "source": [
        "Both `torch.from_numpy` and `torch.as_tensor` share the same memory as numpy array, which means that it just creates a link to the numpy array, and if the value changes in the array, the value of the tensor will change as well, verse versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81ba58f",
      "metadata": {
        "id": "d81ba58f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d4a6a0-5e8f-4694-dfac-ffcff3d09307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original numpy array\t\t\t [1 2 3 4 5 6]\n",
            "tensor created by torch.from_numpy:\t tensor([1, 2, 3, 4, 5, 6])\n",
            "tensor created by torch.as_tensor:\t tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# my_tensor is created by torch.from_numpy from arr\n",
        "# my_tensor2 is create by torch.as_tensor from arr\n",
        "print(\"original numpy array\\t\\t\\t\", arr)\n",
        "print(\"tensor created by torch.from_numpy:\\t\", my_tensor)\n",
        "print(\"tensor created by torch.as_tensor:\\t\", my_tensor2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea19a6ca",
      "metadata": {
        "id": "ea19a6ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7008418-f0b0-425c-cbdc-e8ae460fc35d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original numpy array\t\t\t [100   2   3   4   5   6]\n",
            "tensor created by torch.from_numpy:\t tensor([100,   2,   3,   4,   5,   6])\n",
            "tensor created by torch.as_tensor:\t tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "arr[0] = 100\n",
        "print(\"original numpy array\\t\\t\\t\", arr)\n",
        "print(\"tensor created by torch.from_numpy:\\t\", my_tensor)\n",
        "print(\"tensor created by torch.as_tensor:\\t\", my_tensor2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2f5fa2",
      "metadata": {
        "id": "2b2f5fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a1f3c3-ecb5-49ca-b3d0-c0183235ff80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original numpy array\t\t\t [100  88   3   4   5   6]\n",
            "tensor created by torch.from_numpy:\t tensor([100,  88,   3,   4,   5,   6])\n",
            "tensor created by torch.as_tensor:\t tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "my_tensor[1] = 88\n",
        "print(\"original numpy array\\t\\t\\t\", arr)\n",
        "print(\"tensor created by torch.from_numpy:\\t\", my_tensor)\n",
        "print(\"tensor created by torch.as_tensor:\\t\", my_tensor2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50b94ba",
      "metadata": {
        "id": "a50b94ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fdc6790-08b8-4cce-dc7b-2d54bb2f3061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original numpy array\t\t\t [100  88   3   4   5   6]\n",
            "tensor created by torch.from_numpy:\t tensor([100,  88,   3,   4,   5,   6])\n",
            "tensor created by torch.as_tensor:\t tensor([ 0.,  1.,  2., 66.,  4.,  5.,  6.,  7.,  8.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "my_tensor2[3] = 66\n",
        "print(\"original numpy array\\t\\t\\t\", arr)\n",
        "print(\"tensor created by torch.from_numpy:\\t\", my_tensor)\n",
        "print(\"tensor created by torch.as_tensor:\\t\", my_tensor2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6a75227",
      "metadata": {
        "id": "b6a75227"
      },
      "source": [
        "However, `torch.as_tensor` ***only*** share the memory when the input is numpy array. If it takes other data types as input, it won't share the memory.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d668a0",
      "metadata": {
        "id": "91d668a0"
      },
      "outputs": [],
      "source": [
        "my_ls = [1,2,3]\n",
        "my_tensor_from_ls = torch.as_tensor(my_ls)\n",
        "print(\"original list\\t\\t\\t\\t\\t\", my_ls)\n",
        "print(\"tensor created by torch.as_tensor from list:\\t\", my_tensor_from_ls)\n",
        "\n",
        "my_ls[0] = 99\n",
        "print(\"After change the list\")\n",
        "print(\"original list\\t\\t\\t\\t\\t\", my_ls)\n",
        "print(\"tensor created by torch.as_tensor from list:\\t\", my_tensor_from_ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b92be9",
      "metadata": {
        "id": "53b92be9"
      },
      "outputs": [],
      "source": [
        "my_val = 10\n",
        "my_tensor_from_val = torch.as_tensor(my_val)\n",
        "print(\"original value\\t\\t\\t\\t\\t\\t\", my_val)\n",
        "print(\"tensor created by torch.as_tensor from scalar value:\\t\", my_tensor_from_val)\n",
        "\n",
        "my_val = 99\n",
        "print(\"After change the value\")\n",
        "print(\"original value\\t\\t\\t\\t\\t\\t\", my_val)\n",
        "print(\"tensor created by torch.as_tensor from scalar value:\\t\", my_tensor_from_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43f6297",
      "metadata": {
        "id": "f43f6297"
      },
      "source": [
        "If we **don't** want them to create the direct link and share the memory with numpy array, we just need a copy of the data, we can use `torch.tensor` or `torch.Tensor`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abf44a5",
      "metadata": {
        "id": "5abf44a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624e2ea5-6bd9-4f59-adaa-f478eefed19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5])\n",
            "tensor([0., 1., 2., 3., 4., 5.])\n",
            "tensor([0, 1, 2, 3, 4, 5])\n"
          ]
        }
      ],
      "source": [
        "# create a new array\n",
        "new_arr = np.arange(0, 6)\n",
        "\n",
        "\n",
        "# create tensor using torch.tensor\n",
        "new_tensor = torch.tensor(new_arr)\n",
        "# create tensor using torch.Tensor\n",
        "new_Tensor = torch.Tensor(new_arr)\n",
        "# create tensor using torch.from_numpy\n",
        "new_tensor_from_np = torch.from_numpy(new_arr)\n",
        "\n",
        "# create tensor using torch.from_numpy\n",
        "\n",
        "\n",
        "# Print the original NumPy array\n",
        "print(new_tensor)\n",
        "# Print the tensor created using torch.from_numpy (shares memory with NumPy array)\n",
        "print(new_Tensor)\n",
        "# Print the tensor created using torch.tensor (creates a copy of data)\n",
        "print(new_tensor_from_np)\n",
        "# Print the tensor created using torch.Tensor (uninitialized if no data is given, but here it copies data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "138b39bf",
      "metadata": {
        "id": "138b39bf"
      },
      "outputs": [],
      "source": [
        "# Modify the first element of the original NumPy array\n",
        "\n",
        "# Print the modified NumPy array\n",
        "\n",
        "# Print the tensor created using torch.from_numpy (will also reflect the change, since it shares memory)\n",
        "\n",
        "# Print the tensor created using torch.tensor (will NOT change, since it makes a copy)\n",
        "\n",
        "# Print the tensor created using torch.Tensor (also does NOT change, since it makes a copy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784e21cd",
      "metadata": {
        "id": "784e21cd"
      },
      "outputs": [],
      "source": [
        "# Change the 4th element (index 3) of the original NumPy array\n",
        "\n",
        "# Print the modified NumPy array\n",
        "\n",
        "# Print the tensor created using torch.from_numpy (will reflect the change due to shared memory)\n",
        "\n",
        "# Print the tensor created using torch.tensor (remains unchanged, copy made earlier)\n",
        "\n",
        "# Print the tensor created using torch.Tensor (remains unchanged, copy made earlier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbde04c1",
      "metadata": {
        "id": "dbde04c1"
      },
      "source": [
        "So there are no memory sharing between the original numpy array and the tensor created by the `torch.tensor`.\n",
        "\n",
        "The difference between `torch.tensor` and `torch.Tensor` is that, `torch.Tensor` is actually an alias for the default tensor type in PyTorch, which is `torch.FloatTensor`, which mean the dtype of tensor create by `torch.Tensor` is always `torch.float32`. But with `torch.tensor`, we can still assign the dtype."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cee3337",
      "metadata": {
        "id": "8cee3337"
      },
      "outputs": [],
      "source": [
        "print(new_Tensor.dtype)\n",
        "print(new_tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555dfcb9",
      "metadata": {
        "id": "555dfcb9"
      },
      "outputs": [],
      "source": [
        "# Create a new tensor from the NumPy array with explicit int64 data type\n",
        "\n",
        "# Print the data type of the new tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c8bc97",
      "metadata": {
        "id": "a8c8bc97"
      },
      "source": [
        "In summary, these are the methods we can use to create the tensor from the numpy array:\n",
        "\n",
        "<br />\n",
        "\n",
        "| Method          | Acceptable Inputs              | Assignable `dtype` | Memory Sharing with numpy | Default `dtype` (if not assigned) |\n",
        "|-----------------|-------------------------------|--------------------|---------------------------|----------------------------------|\n",
        "| `torch.from_numpy` | Numpy arrays only            | No                 | Yes                       | Inferred from numpy array        |\n",
        "| `torch.as_tensor`  | Numpy arrays, lists, etc.    | Yes                | Yes (only with numpy & no dtype conversion) | Inferred from input            |\n",
        "| `torch.tensor`     | Numpy arrays, lists, etc.    | Yes                | No                        | Inferred from input              |\n",
        "| `torch.Tensor`     | Numpy arrays, lists, etc.    | No                 | No                        | `torch.float32`                  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d511859",
      "metadata": {
        "id": "8d511859"
      },
      "source": [
        "#### Create Directly\n",
        "\n",
        "Instead of convert numpy array to tensor, we can also create tendsor from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e340012",
      "metadata": {
        "id": "8e340012"
      },
      "source": [
        "`torch.empty` is used to allocate uninitialized memory for a tensor. The values that occupy the uninitialized memory can be arbitrary and should not be relied upon without first setting or initializing them in some manner. We can regard this as a placeholder for the memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c13aaab",
      "metadata": {
        "id": "3c13aaab"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "506d458f",
      "metadata": {
        "id": "506d458f"
      },
      "source": [
        "Using the values from torch.empty without initialization can lead to unexpected behaviors, as the values are arbitrary.\n",
        "\n",
        "If we want to create a tensor with specific initial values (like zeros or ones), you would use functions like `torch.zeros` or `torch.ones` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a520570e",
      "metadata": {
        "id": "a520570e"
      },
      "outputs": [],
      "source": [
        "# Create a 2x2 tensor filled with zeros\n",
        "\n",
        "# Create a 3x4x5 tensor (3D tensor) filled with zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad6ca225",
      "metadata": {
        "id": "ad6ca225"
      },
      "outputs": [],
      "source": [
        "# Create a 2x2 tensor filled with ones\n",
        "\n",
        "# Create a 3x4x5 tensor (3D tensor) filled with ones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68316ede",
      "metadata": {
        "id": "68316ede"
      },
      "source": [
        "We can also use the function like `torch.arange` to create a range of vector, similary to the numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351b97af",
      "metadata": {
        "id": "351b97af"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba82068",
      "metadata": {
        "id": "8ba82068"
      },
      "outputs": [],
      "source": [
        "             #start, #end,  #step\n",
        "#torch.arange(0,     10,     1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de15bb88",
      "metadata": {
        "id": "de15bb88"
      },
      "outputs": [],
      "source": [
        "#torch.arange(0, 20, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3348c9df",
      "metadata": {
        "id": "3348c9df"
      },
      "source": [
        "And we can use reshape to reshape the above 1D tensor, similary to the numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e349a4",
      "metadata": {
        "id": "d1e349a4"
      },
      "outputs": [],
      "source": [
        "#np.arange(0, 16).reshape(4, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58357c9",
      "metadata": {
        "id": "a58357c9"
      },
      "outputs": [],
      "source": [
        "#torch.arange(0, 12, 1).reshape(3, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152e6776",
      "metadata": {
        "id": "152e6776"
      },
      "source": [
        "Or we can also use the `linspace`, which returns a 1-dimensional tensor of evenly spaced values between two specified start and end values. Similar to the numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adff2f4b",
      "metadata": {
        "id": "adff2f4b"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147ac11a",
      "metadata": {
        "id": "147ac11a"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29158d2d",
      "metadata": {
        "id": "29158d2d"
      },
      "source": [
        "We can use `torch.rand`, `torch.randn` and `torch.randint` to generate tensor with random value.\n",
        "\n",
        "`torch.rand`: Generates a tensor filled with random numbers uniformly distributed between `0` and `1`.\n",
        "\n",
        "`torch.randn`: Generates a tensor filled with random numbers from a standard normal distribution (mean = 0 and variance = 1).\n",
        "\n",
        "`torch.randint`:  a tensor filled with random integers between `low`(inclusive) and `high`(exclusive). If only one value is provided, it assumes between 0 and that value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e470541",
      "metadata": {
        "id": "1e470541"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df2b6f4",
      "metadata": {
        "id": "8df2b6f4"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c829df0",
      "metadata": {
        "id": "5c829df0"
      },
      "outputs": [],
      "source": [
        "              #low  #high   #size\n",
        "#torch.randint(0,    10,    (2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8b1f53",
      "metadata": {
        "id": "df8b1f53"
      },
      "source": [
        "Since we are generating the random value here, similar to `numpy.random.seed()` method, PyTorch also have a method called `torch.manual_seed()`, which is used to set the random seed for generating deterministic random numbers, ensuring reproducibility across runs.\n",
        "\n",
        "Reproducibility is crucial in many areas, especially in scientific experiments, machine learning, and data analysis. By using a consistent seed value for random number generation, researchers and practitioners can ensure that the randomness introduced in their experiments is consistent every time they run them. This makes experiments replicable and results comparable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff72a8bb",
      "metadata": {
        "id": "ff72a8bb"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcb6141",
      "metadata": {
        "id": "4dcb6141"
      },
      "source": [
        "Please note that, for reproducibility in interactive environments like Jupyter Notebooks, it's advisable to set the random seed using `torch.manual_seed()` and immediately generate random numbers **within the same cell**. This practice ensures consistent results every time the cell is executed, guarding against potential inconsistencies that might arise from executing cells out of order or intervening operations that use the random number generator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771f39e4",
      "metadata": {
        "id": "771f39e4"
      },
      "source": [
        "Until now, we can see for PyTorch, most of the methods to generate value are similar to numpy, so if you are familiar with numpy, you can get on this easily."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b42b030",
      "metadata": {
        "id": "7b42b030"
      },
      "source": [
        "In PyTorch, we can also use the `*_like` methods to create tensors with the **same shape**(and, optionally, data type) as a given tensor. These methods are convenient when you want a new tensor that matches the dimensions of an existing tensor but initialized differently.\n",
        "\n",
        "Here are the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c7e22b",
      "metadata": {
        "id": "34c7e22b"
      },
      "outputs": [],
      "source": [
        "# give an example of 3D tensor\n",
        "my_3dtensor = torch.ones(7,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b41e6de",
      "metadata": {
        "id": "2b41e6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b4ad6a-8c0b-436c-d7b0-7be9a8ddb5f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "my_3dtensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73af7c9a",
      "metadata": {
        "id": "73af7c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f288d5-ef57-4a73-985f-a15d2799db53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "my_3dtensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5b6ba5",
      "metadata": {
        "scrolled": true,
        "id": "2d5b6ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e882a89c-a8dd-4c72-8ac4-4c7576364f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.empty_like\n",
            "tensor([[8.1603e-24],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [1.4013e-45],\n",
            "        [0.0000e+00],\n",
            "        [0.0000e+00],\n",
            "        [6.8664e-44]])\n",
            "================================================================================\n",
            "torch.zeros_like\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "================================================================================\n",
            "torch.ones_like\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "================================================================================\n",
            "torch.randint_like\n",
            "tensor([[0.],\n",
            "        [7.],\n",
            "        [1.],\n",
            "        [8.],\n",
            "        [0.],\n",
            "        [7.],\n",
            "        [3.]])\n"
          ]
        }
      ],
      "source": [
        "print(\"torch.empty_like\")\n",
        "print(torch.empty_like(my_3dtensor))\n",
        "print(\"=\"*80)\n",
        "print(\"torch.zeros_like\")\n",
        "print(torch.zeros_like(my_3dtensor))\n",
        "print(\"=\"*80)\n",
        "print(\"torch.ones_like\")\n",
        "print(torch.ones_like(my_3dtensor))\n",
        "print(\"=\"*80)\n",
        "print(\"torch.randint_like\")\n",
        "print(torch.randint_like(my_3dtensor, low=0, high=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c4734c",
      "metadata": {
        "id": "68c4734c"
      },
      "source": [
        "BUT note that if we run `torch.rand_like` and `torch.randn_like` on this example, we will get the data type issue. The function `torch.rand_like()` tries to fill a tensor with random numbers from a uniform distribution in the range `[0, 1)` and `torch.randn_like()` tries to fill with random numbers drawn from a standard normal distribution (with mean 0 and standard deviation 1). This operation requires a **floating-point data type**, but our `my_3dtensor` is **`int64`**.\n",
        "\n",
        "To avoid the error, we can convert the `my_3dtensor` to a floating-point type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85fdca9",
      "metadata": {
        "scrolled": true,
        "id": "c85fdca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de59120e-4cec-4544-85ec-81baa298e01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.rand_like\n",
            "tensor([[0.6810],\n",
            "        [0.2370],\n",
            "        [0.7508],\n",
            "        [0.5054],\n",
            "        [0.0073],\n",
            "        [0.8836],\n",
            "        [0.4782]])\n",
            "================================================================================\n",
            "torch.randn_like\n",
            "tensor([[-0.5484],\n",
            "        [ 1.5349],\n",
            "        [ 0.2008],\n",
            "        [ 0.7759],\n",
            "        [ 0.5869],\n",
            "        [-0.2893],\n",
            "        [-0.9562]])\n"
          ]
        }
      ],
      "source": [
        "# convert to float-point type\n",
        "#your code\n",
        "my_3dtensor_float = my_3dtensor.type(torch.float32)\n",
        "print(\"torch.rand_like\")\n",
        "print(torch.rand_like(my_3dtensor_float))\n",
        "print(\"=\"*80)\n",
        "print(\"torch.randn_like\")\n",
        "print(torch.randn_like(my_3dtensor_float))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce3bed56",
      "metadata": {
        "id": "ce3bed56"
      },
      "source": [
        "However, `torch.randint_like` doesn't have restrictions on the source dtype, it will produce a tensor with the same data type as the source tensor, even if the generated values are integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc5b8a7",
      "metadata": {
        "id": "bcc5b8a7"
      },
      "outputs": [],
      "source": [
        "my_float_tensor = #your code\n",
        "print(my_float_tensor)\n",
        "print(my_float_tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca0ee19",
      "metadata": {
        "id": "dca0ee19"
      },
      "outputs": [],
      "source": [
        "my_generate_tensor = #your code\n",
        "my_generate_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc917ba",
      "metadata": {
        "id": "edc917ba"
      },
      "outputs": [],
      "source": [
        "#my_generate_tensor.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6fcf021",
      "metadata": {
        "id": "d6fcf021"
      },
      "source": [
        "Here is the summary of the `*_like` methods:\n",
        "\n",
        "<br />\n",
        "\n",
        "| Method                      | Description                                                           | Requires Source `dtype`   | Output `dtype`                  |\n",
        "|-----------------------------|-----------------------------------------------------------------------|---------------------------|--------------------------------|\n",
        "| `torch.empty_like(tensor)`  | Generates an uninitialized tensor.                                    | Any                       | Matches source tensor          |\n",
        "| `torch.zeros_like(tensor)`  | Generates a tensor filled with zeros.                                 | Any                       | Matches source tensor          |\n",
        "| `torch.ones_like(tensor)`   | Generates a tensor filled with ones.                                  | Any                       | Matches source tensor          |\n",
        "| `torch.rand_like(tensor)`   | Generates uniform random numbers between 0 and 1.                     | Floating-point            | Matches source tensor          |\n",
        "| `torch.randn_like(tensor)`  | Generates random numbers from a standard normal distribution.         | Floating-point            | Matches source tensor          |\n",
        "| `torch.randint_like(tensor, low, high)` | Generates random integers between `low` and `high` (exclusive). | Any                       | Matches source tensor, but filled with integers |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d83cab",
      "metadata": {
        "id": "83d83cab"
      },
      "source": [
        "There are another two methods you may need to use to create the tensor:\n",
        "\n",
        "`torch.full` - It creates a tensor of size size with all values set to fill_value.\n",
        "\n",
        "`torch.eye` - It creates a 2-D tensor with ones on the diagonal and zeros elsewhere, essentially an identity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ada50a2",
      "metadata": {
        "id": "4ada50a2"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1f771d",
      "metadata": {
        "id": "ce1f771d"
      },
      "outputs": [],
      "source": [
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e25807",
      "metadata": {
        "id": "85e25807"
      },
      "source": [
        "In additional, all the `torch.*` method we metioned here supports the GPU computation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0866638e",
      "metadata": {
        "id": "0866638e"
      },
      "source": [
        "## Tensor Properties and Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b840f2e5",
      "metadata": {
        "id": "b840f2e5"
      },
      "source": [
        "### Tensor Properties\n",
        "\n",
        "There are several properties that you can leverage to better understand and work with them.\n",
        "\n",
        "\n",
        " - `dtype`: The data type of a tensor tells us the type of data contained in it, such as integers or floats.\n",
        "\n",
        " - `shape`: The shape of a tensor provides the dimensions of the tensor.\n",
        "\n",
        " - `size()`: The `size()` method returns the shape of the tensor. It's an alternative to `shape`.\n",
        "\n",
        " - `dim()`: This tells you the number of dimensions present in the tensor.\n",
        "\n",
        " - `max()/min()`: Get the maximum or minimum value from the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05ab8b2",
      "metadata": {
        "id": "d05ab8b2"
      },
      "outputs": [],
      "source": [
        "# dtype\n",
        "\n",
        "# Create a tensor with float values\n",
        "\n",
        "# Print the data type of the float tensor\n",
        "\n",
        "# Create a tensor with integer values\n",
        "\n",
        "# Print the data type of the integer tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38154cd2",
      "metadata": {
        "id": "38154cd2"
      },
      "outputs": [],
      "source": [
        "# Create a 2D tensor (matrix) with 3 rows and 2 columns\n",
        "\n",
        "# Create a 3D tensor (2 matrices, each of size 2x2)\n",
        "\n",
        "# Print the 2D tensor\n",
        "\n",
        "# Print a separator line for clarity\n",
        "\n",
        "# Print the 3D tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0edf3e",
      "metadata": {
        "id": "dd0edf3e"
      },
      "outputs": [],
      "source": [
        "# shape\n",
        "print(f\"Shape of tensor_2d: {tensor_2d.shape}\")\n",
        "print(f\"Shape of tensor_3d: {tensor_3d.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225d85b8",
      "metadata": {
        "id": "225d85b8"
      },
      "outputs": [],
      "source": [
        "# size()\n",
        "print(f\"Size of tensor_2d: {tensor_2d.size()}\")\n",
        "print(f\"Size of tensor_3d: {tensor_3d.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4372c792",
      "metadata": {
        "id": "4372c792"
      },
      "outputs": [],
      "source": [
        "# dim()\n",
        "print(f\"Dimensions of tensor_2d: {tensor_2d.dim()}\")\n",
        "print(f\"Dimensions of tensor_3d: {tensor_3d.dim()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ee703f",
      "metadata": {
        "id": "d3ee703f"
      },
      "outputs": [],
      "source": [
        "# min/max\n",
        "print(f\"Max value: {tensor_3d.max()}\")\n",
        "print(f\"Min value: {tensor_3d.min()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3194229a",
      "metadata": {
        "id": "3194229a"
      },
      "source": [
        "### Tensor Indexing and Slicing\n",
        "Tensor indexing and slicing is a way to access or modify specific portions of a tensor. It's similar to Python list indexing with some added functionalities to cater to multi-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ee2df7",
      "metadata": {
        "id": "d4ee2df7"
      },
      "source": [
        "#### Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a926603",
      "metadata": {
        "id": "7a926603"
      },
      "source": [
        "The basic indexing is similar to the python list or numpy array. we can either indexing a single item or use the negative indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36c358a",
      "metadata": {
        "id": "e36c358a"
      },
      "outputs": [],
      "source": [
        "tensor1d = torch.tensor([0, 1, 2, 3, 4])\n",
        "\n",
        "# Indexing a single item\n",
        "\n",
        "\n",
        "# Negative indexing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbceae8d",
      "metadata": {
        "id": "dbceae8d"
      },
      "outputs": [],
      "source": [
        "tensor2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "print(tensor2d)\n",
        "print()\n",
        "# Accessing a specific row\n",
        "#your code\n",
        "  # Outputs: [4, 5, 6]\n",
        "\n",
        "# Accessing a specific element\n",
        "#your code  # Outputs: 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8061b1",
      "metadata": {
        "id": "ba8061b1"
      },
      "outputs": [],
      "source": [
        "tensor3d = torch.tensor([\n",
        "                         [[ 0,  1,  2,  3], [ 4,  5,  6,  7]],\n",
        "                         [[ 8,  9, 10, 11], [12, 13, 14, 15]],\n",
        "                         [[16, 17, 18, 19], [20, 21, 22, 23]]\n",
        "                        ])\n",
        "\n",
        "print(tensor3d)\n",
        "\n",
        "# Access the first batch\n",
        "#your code\n",
        "\n",
        "# Access the second element of the first batch\n",
        "#your code\n",
        "\n",
        "# Access the third element of the second column of the second batch\n",
        "#your code\n",
        "# Extract a subarray from the third batch\n",
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c594cd1",
      "metadata": {
        "id": "4c594cd1"
      },
      "source": [
        "#### Slicing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e2f607",
      "metadata": {
        "id": "56e2f607"
      },
      "source": [
        "Slicing in tensors allows us to extract continuous segments, akin to subsetting lists or arrays in Python, enabling precise and flexible data partitioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc2d163",
      "metadata": {
        "id": "9cc2d163"
      },
      "outputs": [],
      "source": [
        "# Create a simple 1D tensor with values 0 to 4\n",
        "\n",
        "# Slice from index 1 to 3 (end index 4 is exclusive) → gives [1, 2, 3]\n",
        "\n",
        "# Slice from index 1 to 3 with step size 2 → picks every second element → gives [1, 3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cd914b",
      "metadata": {
        "id": "c8cd914b"
      },
      "outputs": [],
      "source": [
        "# Create a 2D tensor (matrix) with 3 rows and 2 columns\n",
        "\n",
        "# Create a 3D tensor (2 matrices, each of size 2x2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e5ef08",
      "metadata": {
        "id": "d8e5ef08"
      },
      "outputs": [],
      "source": [
        "print(f\"The 2D tensor \\n{tensor_2d}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Selecting the row for 2d tensor\n",
        "\n",
        "\n",
        "# Selecting the column for 2d tensor\n",
        "\n",
        "\n",
        "# Selecting multiple rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6bb662",
      "metadata": {
        "id": "0c6bb662"
      },
      "outputs": [],
      "source": [
        "print(f\"The 3D tensor \\n{tensor_3d}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Selecting a 2D matrix from 3d tensor\n",
        "\n",
        "\n",
        "# Selecting a row across matrices\n",
        "\n",
        "\n",
        "# Selecting a column across matrices\n",
        "\n",
        "\n",
        "# Selecting a single value\n",
        "\n",
        "\n",
        "# Selecting a sub-tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6e3e40",
      "metadata": {
        "id": "3b6e3e40"
      },
      "source": [
        "The `:` here in the `[:, :, 0]` is just a placeholder, that signifies **\"take everything along this dimension.\"** It is used for slicing to specify that all elements along a given dimension should be included. This is very much like the behavior of slicing in Python lists and NumPy arrays.\n",
        "\n",
        "And in the *Selecting the first row across all matrices* example, the `[:, 0]` is equal to `[:, 0, :]`. In PyTorch (as well as in NumPy), when you're slicing into multi-dimensional arrays, if you don't provide indices for all the dimensions, the missing dimensions are considered as complete slices (`:`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02bb4243",
      "metadata": {
        "id": "02bb4243"
      },
      "source": [
        "#### Use Cases with Real-world Scenarios\n",
        "\n",
        "1. Extracting the First Token's Embedding in BERT\n",
        "\n",
        "    In transformers like BERT, the embedding of the first token (often `[CLS]`) is typically used as the aggregate sequence representation.\n",
        "\n",
        "    For example, we want to use the output of BERT to do the text classification, here is an example for getting the text representation\n",
        "\n",
        "    ```\n",
        "    # Assume `outputs` is the output from BERT model, having shape [batch_size, sequence_length, hidden_size]\n",
        "    cls_embeddings = outputs[:, 0, :]\n",
        "    ```\n",
        "    \n",
        "<br />\n",
        "\n",
        "2. Splitting a Tensor into Batches\n",
        "\n",
        "    In mini-batch training, you split your data into batches.\n",
        "    ```\n",
        "    data = torch.randn(100, 512)\n",
        "    batch_size = 32\n",
        "\n",
        "    for i in range(0, 100, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "    ```\n",
        "\n",
        "<br />\n",
        "\n",
        "3. Image Downscaling\n",
        "\n",
        "    Downscale an image by taking every nth pixel.\n",
        "    ```\n",
        "    image = torch.randn(1, 3, 256, 256)\n",
        "    downscaled = image[:, :, ::2, ::2]\n",
        "    ```\n",
        "    \n",
        "    <details>\n",
        "        <summary>>Some explaination(Click to expand)</summary>\n",
        "    \n",
        "        `1` - Represents the batch size. This tensor contains one image.\n",
        "    \n",
        "        `3` - Represents the number of channels (typically for RGB images, where 3 channels correspond to Red, Green, and Blue).\n",
        "    \n",
        "        `256` - Height of the image.\n",
        "    \n",
        "        `256` - Width of the image.\n",
        "    \n",
        "        `:` - The first `:` means we are taking all elements along the batch dimension. In this case, it's just one image.\n",
        "    \n",
        "        `:` - The second `:` means we are taking all channels of the image (R, G, B).\n",
        "    \n",
        "        `::2` - The first `::2` means we are taking every second pixel in the height dimension. It effectively halves the height of the image.\n",
        "    \n",
        "        `::2` - The first `::2` means we are taking every second pixel in the width dimension. It effectively halves the width of the image.\n",
        "    \n",
        "    </details>\n",
        "\n",
        "<br />\n",
        "\n",
        "4. Sequence Padding and Truncation in NLP\n",
        "\n",
        "    Pad (or truncate) sequences to a fixed length for batching.\n",
        "    ```\n",
        "    max_length = 50\n",
        "    sequence = torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "    if len(sequence) < max_length:\n",
        "        padded_sequence = torch.cat([sequence, torch.zeros(max_length - len(sequence))])\n",
        "    else:\n",
        "        padded_sequence = sequence[:max_length]\n",
        "\n",
        "    ```\n",
        "    \n",
        "<br />\n",
        "\n",
        "5. Selecting Specific Channels in an Image\n",
        "\n",
        "    From an RGB image, extract only the Red channel.\n",
        "    ```\n",
        "    image = torch.randn(1, 3, 256, 256)\n",
        "    red_channel = image[:, 0, :, :]\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a4d4b9",
      "metadata": {
        "id": "29a4d4b9"
      },
      "source": [
        "### Tensor Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c710337b",
      "metadata": {
        "id": "c710337b"
      },
      "source": [
        "#### Basic Computations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e56a129",
      "metadata": {
        "id": "9e56a129"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3], dtype=torch.float)\n",
        "b = torch.tensor([4, 5, 6], dtype=torch.float)\n",
        "print(f\"Tensor a: {a}\")\n",
        "print(f\"Tensor b: {b}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Addition\n",
        "# also we can use torch.add(a, b) or a+b\n",
        "#your code\n",
        "\n",
        "# Subtraction\n",
        "# also we can use torch.sub(a, b) or a-b\n",
        "#your code\n",
        "\n",
        "# Multiplication (Element-wise)\n",
        "# also we can use torch.mul(a, b) or a*b\n",
        "#your code\n",
        "\n",
        "# Division (Element-wise)\n",
        "# also we can use torch.div(a, b)  or a/b\n",
        "#your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa72c000",
      "metadata": {
        "id": "fa72c000"
      },
      "source": [
        "We can find that after all these computations, the value for `a` and `b` won't change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3230d6bb",
      "metadata": {
        "id": "3230d6bb"
      },
      "outputs": [],
      "source": [
        "print(f\"Tensor a: {a}\")\n",
        "print(f\"Tensor b: {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b806b5",
      "metadata": {
        "id": "56b806b5"
      },
      "source": [
        "So this means that if we want to save the value after the computation, we always need to assign a new variable. Sometimes, we want to keep the changes in the original variable for memory and performance considerations. So, in PyTorch, we can use the in-place operation to do this.\n",
        "\n",
        "In-place operations are a crucial aspect of PyTorch's tensor operations, especially when considering memory management and performance optimization.\n",
        "\n",
        "In PyTorch, in-place operations are methods that have a `_` suffix, like `add_()`, `mul_()`, etc. These operations modify the original tensor directly and do not create a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6894a711",
      "metadata": {
        "id": "6894a711"
      },
      "outputs": [],
      "source": [
        "# In-place addition\n",
        "a.add_(b)\n",
        "#your code\n",
        "\n",
        "# In-place multiplication\n",
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b956b6",
      "metadata": {
        "id": "59b956b6"
      },
      "outputs": [],
      "source": [
        "# In-place subtraction\n",
        "a.sub_(b)\n",
        "#your code\n",
        "\n",
        "# In-place division\n",
        "a.div_(b)\n",
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0c0462",
      "metadata": {
        "id": "7b0c0462"
      },
      "outputs": [],
      "source": [
        "print(f\"Tensor b won't be affected: {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30bbc45",
      "metadata": {
        "id": "e30bbc45"
      },
      "source": [
        "#### Matrix Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18eed71e",
      "metadata": {
        "id": "18eed71e"
      },
      "outputs": [],
      "source": [
        "M = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\n",
        "N = torch.tensor([[9, 8, 7], [6, 5, 4]], dtype=torch.float)\n",
        "print(f\"Matrix M: \\n{M}\")\n",
        "print(f\"Matrix N: \\n{N}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Matrix Multiplication\n",
        "# also we can use  M.mm(N)\n",
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1977e8c2",
      "metadata": {
        "id": "1977e8c2"
      },
      "source": [
        "However, `torch.mm()` and `M.mm(N)` **only** works for **2D tensor**\n",
        "\n",
        "We can use `torch.matmul()` or `M.matmul(N)` for higher dimension, the `matmul` support the not only **2D tensor**, but also **higher dimension**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d715285b",
      "metadata": {
        "id": "d715285b"
      },
      "outputs": [],
      "source": [
        "A_batch = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "B_batch = torch.tensor([[[2, 0], [0, 2]], [[1, 1], [1, 1]]])\n",
        "\n",
        "#your code\n",
        "\n",
        "# we can also use A_batch @ B_batch\n",
        "#your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78721934",
      "metadata": {
        "id": "78721934"
      },
      "source": [
        "In the above result:\n",
        "\n",
        "The first batch `[[2, 4], [6, 8]]` of the result comes from the matrix multiplication of the first matrix of `A_batch` and `B_batch`, so `[[1, 2], [3, 4]] X [[2, 0], [0, 2]]`\n",
        "\n",
        "The second batch `[[11, 11], [15, 15]]` of the result comes from the matrix multiplication of the second matrix of `A_batch` and `B_batch`, so `[[5, 6], [7, 8]] X [[1, 1], [1, 1]]]`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define two 2D vectors\n",
        "A = np.array([2, 3])\n",
        "B = np.array([-2, -3])\n",
        "\n",
        "# Calculate the dot product\n",
        "#your code\n",
        "\n",
        "# Plot the vectors\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.quiver(0, 0, A[0], A[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Vector A')\n",
        "plt.quiver(0, 0, B[0], B[1], angles='xy', scale_units='xy', scale=1, color='red', label='Vector B')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-4, 4)\n",
        "plt.axhline(0, color='black',linewidth=0.5)\n",
        "plt.axvline(0, color='black',linewidth=0.5)\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title(\"Dot Product of Vectors\")\n",
        "plt.text(0.5, 1, f\"Dot Product: {dot_product}\", fontsize=10, bbox=dict(facecolor='none', edgecolor='black'))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nI66EtXoMLdi"
      },
      "id": "nI66EtXoMLdi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# # Define two vectors\n",
        "# A = np.array([2, 3, 4])\n",
        "# B = np.array([5, 6, 7])\n",
        "\n",
        "# # Compute the cross product\n",
        "# cross_product = np.cross(A, B)\n",
        "\n",
        "# # Create a figure and 3D axis\n",
        "# fig = plt.figure(figsize=(8, 8))\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# # Plot the original vectors\n",
        "# ax.quiver(0, 0, 0, A[0], A[1], A[2], color='blue', label='Vector A')\n",
        "# ax.quiver(0, 0, 0, B[0], B[1], B[2], color='red', label='Vector B')\n",
        "\n",
        "# # Plot the cross product vector\n",
        "# ax.quiver(0, 0, 0, cross_product[0], cross_product[1], cross_product[2], color='green', label='Cross Product')\n",
        "\n",
        "# # Set plot limits\n",
        "# ax.set_xlim([0, max(A[0], B[0], cross_product[0])])\n",
        "# ax.set_ylim([0, max(A[1], B[1], cross_product[1])])\n",
        "# ax.set_zlim([0, max(A[2], B[2], cross_product[2])])\n",
        "\n",
        "# # Set labels and title\n",
        "# ax.set_xlabel('X')\n",
        "# ax.set_ylabel('Y')\n",
        "# ax.set_zlabel('Z')\n",
        "# ax.set_title('Cross Product Visualization')\n",
        "\n",
        "# # Show legend\n",
        "# ax.legend()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "3lBEtYwfMLWF"
      },
      "id": "3lBEtYwfMLWF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADmhuWjGMLGe"
      },
      "id": "ADmhuWjGMLGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2y7QdQstMLAw"
      },
      "id": "2y7QdQstMLAw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cf7c657",
      "metadata": {
        "id": "6cf7c657"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3], dtype=torch.float)\n",
        "b = torch.tensor([4, 5, 6], dtype=torch.float)\n",
        "\n",
        "print(f\"Tensor a: {a}\")\n",
        "print(f\"Tensor b: {b}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Dot Product\n",
        "# also we can use a.dot(b)\n",
        "dot_product = torch.dot(a, b)\n",
        "print(f\"Dot Product of a and b: {dot_product}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Norm (L2 norm by default)\n",
        "# also we can use a.norm()\n",
        "norm_result = torch.norm(a)\n",
        "print(f\"Norm of a (L2 by default): {norm_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc777fc",
      "metadata": {
        "id": "afc777fc"
      },
      "source": [
        "The inputs to `torch.dot` **must be 1D tensors**. And the **sizes** of the two tensors **must be the same**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23f191c",
      "metadata": {
        "id": "d23f191c"
      },
      "source": [
        "You can compute various norms (L1, L2, etc.) using `torch.norm`.\n",
        "\n",
        "It can operate on multi-dimensional tensors. For a matrix, it will compute the Frobenius norm by default.\n",
        "\n",
        "If you want to compute the norm across specific dimensions, you can specify them using the `dim` parameter.\n",
        "\n",
        "If applied to a multi-dimensional tensor without specifying any dimensions, it will treat the tensor as a flattened 1D tensor and compute the norm accordingly.\n",
        "\n",
        "Here is the official document for torch.norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2efede",
      "metadata": {
        "scrolled": true,
        "id": "9b2efede"
      },
      "outputs": [],
      "source": [
        "help(torch.norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be845c2",
      "metadata": {
        "id": "6be845c2"
      },
      "source": [
        "#### Conversions and Reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c1ea91",
      "metadata": {
        "id": "83c1ea91"
      },
      "outputs": [],
      "source": [
        "# Sample tensors for demonstration\n",
        "a = torch.tensor([1, 2, 3], dtype=torch.float)\n",
        "\n",
        "print(f\"Tensor a: {a}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Convert to Numpy\n",
        "#your code\n",
        "\n",
        "# Reshape\n",
        "#your code\n",
        "\n",
        "# View (similar to reshape)\n",
        "#your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3aa91f1",
      "metadata": {
        "id": "c3aa91f1"
      },
      "source": [
        " Both `view` and `reshape` return tensors that share the same underlying data with the original tensor, unless a data copy is made. This means that if you modify the original tensor, any tensors derived from it via `view` or `reshape` (when they don't involve a data copy) will also reflect those changes. Similarly, if you modify the reshaped or viewed tensor, the original tensor will change as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece39af0",
      "metadata": {
        "id": "ece39af0"
      },
      "outputs": [],
      "source": [
        "print(f\"Orignal Tensor a: {a}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# modify the original tensor\n",
        "#your code\n",
        "\n",
        "print(\"Modified original tensor:\")\n",
        "print(a)\n",
        "print(\"=\"*40)\n",
        "print(\"Viewed tensor:\")\n",
        "print(viewed_tensor)\n",
        "print(\"=\"*40)\n",
        "print(\"Reshaped tensor:\")\n",
        "print(reshaped_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c092078",
      "metadata": {
        "id": "9c092078"
      },
      "outputs": [],
      "source": [
        "print(f\"Orignal Tensor a: {a}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# modify the reshaped or viewed tensor\n",
        "#your code\n",
        "\n",
        "print(\"Modified original tensor:\")\n",
        "print(a)\n",
        "print(\"=\"*40)\n",
        "print(\"Viewed tensor:\")\n",
        "print(viewed_tensor)\n",
        "print(\"=\"*40)\n",
        "print(\"Reshaped tensor:\")\n",
        "print(reshaped_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "789733ba",
      "metadata": {
        "id": "789733ba"
      },
      "outputs": [],
      "source": [
        "# Item (converts a 1-element tensor to a Python scalar)\n",
        "one_element_tensor = torch.tensor([[[[5]]]])\n",
        "scalar = one_element_tensor.item()\n",
        "print(f\"Scalar value from one_element_tensor: {scalar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f54ab8",
      "metadata": {
        "id": "66f54ab8"
      },
      "source": [
        "`item` **only** works with tensor which **only has one element**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13f5cfa",
      "metadata": {
        "id": "c13f5cfa"
      },
      "source": [
        "#### Transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adff8d5",
      "metadata": {
        "id": "9adff8d5"
      },
      "outputs": [],
      "source": [
        "M = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\n",
        "#your code\n",
        "print(f\"Matrix M: \\n{M}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "\n",
        "print(f\"Transposed Matrix of M: \\n{transposed_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a78cf7a",
      "metadata": {
        "id": "7a78cf7a"
      },
      "source": [
        "We can also use the `transpose` and `permute` to do the transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697c823e",
      "metadata": {
        "id": "697c823e"
      },
      "outputs": [],
      "source": [
        "transposed_tensor = #your code  # Swapping dimension 0 with dimension 1\n",
        "print(f\"Transposed Matrix of M: \\n{transposed_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36bce50",
      "metadata": {
        "id": "b36bce50"
      },
      "outputs": [],
      "source": [
        "#permuted_tensor = M.permute(1, 0)\n",
        "#print(f\"Transposed Matrix of M: \\n{transposed_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb9a3ab",
      "metadata": {
        "id": "9eb9a3ab"
      },
      "source": [
        "We can see the order of the parameter for `permute` and `transposed` is a different, the reason is:\n",
        " - transpose(dim0, dim1):\n",
        "    - This method swaps two specified dimensions (dim0 and dim1).\n",
        "    - For a 2D tensor (matrix), if you want to transpose it, you swap the rows and columns, so you use transpose(0, 1).\n",
        "\n",
        "- permute(*dims):\n",
        "    - This method allows for more general rearrangements of dimensions.\n",
        "    - The argument to permute is a sequence of dimension indices.\n",
        "    - For a 2D tensor, permute(1, 0) means you want the second dimension (index 1) to become the first, and the first dimension (index 0) to become the second."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d882cc",
      "metadata": {
        "id": "53d882cc"
      },
      "source": [
        "Here is the comparison between these three transpose methods:\n",
        "\n",
        "| Function   | Application              | Syntax                                     | Limitations                                                                                      | Use-Case                             |\n",
        "|------------|--------------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------|\n",
        "| `t`        | Transposing matrices     | `torch.t(tensor)` or `tensor.t()`          | Only works for 2D tensors.                                                                       | Quick transposition of 2D matrices.   |\n",
        "| `transpose`| Transposing dimensions   | `torch.transpose(tensor, dim0, dim1)`<br>or `tensor.transpose(dim0, dim1)`| Requires specifying two dimensions to swap.                                                     | Transposing any two specific dimensions. Useful for 2D tensors and beyond.   |\n",
        "| `permute`  | Rearranging dimensions   | `tensor.permute(*dims)`                    | Requires specifying all dimensions in the new order.                                            | Rearranging order of all dimensions, especially useful for tensors with more than 2 dimensions. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de0a389",
      "metadata": {
        "id": "7de0a389"
      },
      "source": [
        "Example for `transpose` and `permute` in higher dimension tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b84b440",
      "metadata": {
        "scrolled": true,
        "id": "0b84b440"
      },
      "outputs": [],
      "source": [
        "X = torch.arange(0,24).reshape(2, 3, 4)\n",
        "\n",
        "print(f\"The original 3D tensor:\\n {X}\")\n",
        "print(f\"\\nThe shape of original 3D tensor:{X.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# switch the 0th and 2nd dimensions\n",
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bc566f",
      "metadata": {
        "scrolled": true,
        "id": "f8bc566f"
      },
      "outputs": [],
      "source": [
        "print(f\"The original 3D tensor:\\n {X}\")\n",
        "print(f\"\\nThe shape of original 3D tensor:{X.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# switch the 1st and 2nd dimensions\n",
        "#your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558a1797",
      "metadata": {
        "scrolled": true,
        "id": "558a1797"
      },
      "outputs": [],
      "source": [
        "print(f\"The original 3D tensor:\\n {X}\")\n",
        "print(f\"\\nThe shape of original 3D tensor:{X.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# rearrange the order of dimensions\n",
        "print(\"rearrange the order of dimensions using permute\")\n",
        "permuted_tensor = #your code\n",
        "print(f\"The transposed 3D tensor:\\n {permuted_tensor}\")\n",
        "print(f\"\\nThe shape of transposed 3D tensor:{permuted_tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c4cbd4",
      "metadata": {
        "id": "17c4cbd4"
      },
      "outputs": [],
      "source": [
        "print(f\"The original 3D tensor:\\n {X}\")\n",
        "print(f\"\\nThe shape of original 3D tensor:{X.shape}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# rearrange the order of dimensions\n",
        "print(\"rearrange the order of dimensions using permute\")\n",
        "permuted_tensor2 = #your code\n",
        "print(f\"The transposed 3D tensor:\\n {permuted_tensor2}\")\n",
        "print(f\"\\nThe shape of transposed 3D tensor:{permuted_tensor2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e33b894",
      "metadata": {
        "id": "6e33b894"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ae456b64",
        "3194229a",
        "c710337b",
        "e30bbc45"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}